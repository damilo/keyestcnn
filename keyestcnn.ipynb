{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio key estimation of digital music with CNNs\n",
    "Udacity Machine Learning Nanodegree - Capstone project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "The project is structured as stated in section 'Project Design' of the Capstone project proposal.\n",
    "\n",
    "_\n",
    "<pre>\n",
    "<a href='#Data-Preprocessing'>Data Preprocessing</a>\n",
    "  <a href='#Million-Song-Dataset'>Million Song Dataset</a> - selection of appropriate songs, separate jupyter notebook\n",
    "  <a href='#Signal-Processing-and-Feature-Extraction'>Signal Processing and Feature Extraction</a> - separate jupyter notebook\n",
    "\n",
    "<a href='#Model-Preparation'>Model Preparation</a>\n",
    "  <a href='#Load-and-preprocess-data'>Load and preprocess data</a> - read spectrogram images, conversion to tensors\n",
    "  <a href='#Split-data-into-train-and-test-set'>Splitting data into training/testing sets</a>\n",
    "  <a href='#Model-architecture'>CNN model architecture</a>\n",
    "  <a href='#Model-parameter'>CNN model parameter</a>\n",
    "\n",
    "<a href='#Model-Training-and-Evaluation'>Model Training and Evaluation</a>\n",
    "  <a href='#Model-training'>Model training</a>\n",
    "  <a href='#Model-evaluation-and-comparison'>Model evaluation and comparison</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current version of the project is working, but\n",
    "\n",
    "the project is still ongoing...\n",
    "\n",
    "discussion and remarks of what to do can be found in section\n",
    "\n",
    "<a href='#reasons-/-todo'>reasons / todo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Million Song Dataset\n",
    "- utilized to select appropriate song samples\n",
    "- holds information about key and mode per song (targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juypter Notebook <a href='./00.hlp/msd/msd.ipynb'>msd</a>\n",
    "\n",
    "outputs: csv file *songs_conf=75_tracks_filt.csv*, which holds all songs with key confidence and mode confidence > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>track_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.852</td>\n",
       "      <td>114.493</td>\n",
       "      <td>TRMMMGL128F92FD6AB</td>\n",
       "      <td>SOHSSPG12A8C144BE0</td>\n",
       "      <td>Clifford T. Ward</td>\n",
       "      <td>Mad About You</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key  key_confidence  mode  mode_confidence    tempo            track_id  \\\n",
       "0    7           0.896     1            0.852  114.493  TRMMMGL128F92FD6AB   \n",
       "\n",
       "              song_id       artist_name     song_title  \n",
       "0  SOHSSPG12A8C144BE0  Clifford T. Ward  Mad About You  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of records: 47913\n"
     ]
    }
   ],
   "source": [
    "# LIST SELECTED SONGS\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "selsongsfile = os.path.join ('00.hlp', 'msd', 'songs_conf=75_tracks_filt.csv')\n",
    "selsongs = pd.read_csv (selsongsfile, header=0, index_col=0)\n",
    "display (selsongs.head (1))\n",
    "print ('[i] number of records:', len (selsongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AUDIO DATASET\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_audio')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_audio_data = datasets.load_files (container_path=container_path,\n",
    "                                      description=description,\n",
    "                                      load_content=load_content,\n",
    "                                      random_state=PARAM_RND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>track_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>178.571</td>\n",
       "      <td>TRMCBGQ128F425C0A8</td>\n",
       "      <td>SOHGQTL12A8C139348</td>\n",
       "      <td>Jo-El Sonnier</td>\n",
       "      <td>The Back Door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29273</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889</td>\n",
       "      <td>132.947</td>\n",
       "      <td>TRUXMRH128F92F7A43</td>\n",
       "      <td>SOGTREL12AB0181DC8</td>\n",
       "      <td>Blues Company</td>\n",
       "      <td>Dark Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21568</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.847</td>\n",
       "      <td>153.823</td>\n",
       "      <td>TRAVHZZ128F424BEDA</td>\n",
       "      <td>SONSGIT12A8C138856</td>\n",
       "      <td>Heldon</td>\n",
       "      <td>In Wake of King Fripp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46577</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965</td>\n",
       "      <td>95.543</td>\n",
       "      <td>TRYFBHO128F422B190</td>\n",
       "      <td>SONEEWO12A58A7F42A</td>\n",
       "      <td>Tom Rush</td>\n",
       "      <td>Rockport Sunday [Remastered Album Version]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>101.146</td>\n",
       "      <td>TRRJCHZ128F4270606</td>\n",
       "      <td>SOANZZR12A8C134DE5</td>\n",
       "      <td>Vivian Khor</td>\n",
       "      <td>Ancient Wanderer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  key_confidence  mode  mode_confidence    tempo  \\\n",
       "311      7             1.0     1            1.000  178.571   \n",
       "29273    0             1.0     0            0.889  132.947   \n",
       "21568    3             1.0     0            0.847  153.823   \n",
       "46577   11             1.0     1            0.965   95.543   \n",
       "10322    1             1.0     1            1.000  101.146   \n",
       "\n",
       "                 track_id             song_id    artist_name  \\\n",
       "311    TRMCBGQ128F425C0A8  SOHGQTL12A8C139348  Jo-El Sonnier   \n",
       "29273  TRUXMRH128F92F7A43  SOGTREL12AB0181DC8  Blues Company   \n",
       "21568  TRAVHZZ128F424BEDA  SONSGIT12A8C138856         Heldon   \n",
       "46577  TRYFBHO128F422B190  SONEEWO12A58A7F42A       Tom Rush   \n",
       "10322  TRRJCHZ128F4270606  SOANZZR12A8C134DE5    Vivian Khor   \n",
       "\n",
       "                                       song_title  \n",
       "311                                 The Back Door  \n",
       "29273                                    Dark Day  \n",
       "21568                       In Wake of King Fripp  \n",
       "46577  Rockport Sunday [Remastered Album Version]  \n",
       "10322                            Ancient Wanderer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of records: 312\n",
      "[i] min of: key_confidence = 0.825 , mode_confidence = 0.777\n",
      "[i] tempo: min = 0.0 , max = 248.32299999999998\n"
     ]
    }
   ],
   "source": [
    "# FYI: LIST SOME OF THE USED SONGS\n",
    "filenames = list (os.path.basename (filepath) for filepath in src_audio_data['filenames'])\n",
    "usedsongs_track_id = list (os.path.splitext (fn)[0] for fn in filenames)\n",
    "usedsongs = selsongs.query ('track_id in @usedsongs_track_id')\n",
    "\n",
    "display (usedsongs.sample(5))\n",
    "print ('[i] number of records:', len (usedsongs))\n",
    "print ('[i] min of: key_confidence =', usedsongs['key_confidence'].min (), ',', \\\n",
    "       'mode_confidence =', usedsongs['mode_confidence'].min ())\n",
    "print ('[i] tempo: min =', usedsongs['tempo'].min (), ',', \\\n",
    "       'max =', usedsongs['tempo'].max ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- save list of used songs\n",
    "usedsongs.to_csv ('usedsongs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- save list of unused songs\n",
    "unusedsongs = selsongs.drop ((usedsongs.index.values))\n",
    "unusedsongs.to_csv ('unusedsongs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing and Feature Extraction\n",
    "- create spectrograms of audio files with discrete Fourier transform (DFT)\n",
    "- save spectrograms as images for further use in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juypter Notebook <a href='./00.hlp/fft/fft.ipynb'>fft</a>\n",
    "\n",
    "ouptuts: spectrograms (png images) of audio files with same folder structure as *src_audio* in new container path named *src_spectro*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of a spectrogram image**\n",
    "\n",
    "<img src ='./src_spectro/7-0/TREDRTV12903D03829.png' align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Since there aren't enough samples to proper train the classifier, image augmentation is used.\n",
    "\n",
    "Below jupyter notebook does the work.\n",
    "\n",
    "Juypter Notebook <a href='./00.hlp/trnsp/trnsp.ipynb'>transposing songs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'filenames', 'target', 'target_names'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD SPECTROGRAM FILENAMES\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_spectro')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_spectro_data = datasets.load_files (container_path=container_path,\n",
    "                                        description=description,\n",
    "                                        load_content=load_content,\n",
    "                                        random_state=PARAM_RND_STATE)\n",
    "src_spectro_data.keys ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] example of loaded spectrogram file data:\n",
      "    spectrogram image name: src_spectro/2-0/TREFDWU128F9303DF8.png\n",
      "    spectrogram image key-mode pair: 2-0 = Em = target class 8\n"
     ]
    }
   ],
   "source": [
    "print ('[i] example of loaded spectrogram file data:')\n",
    "print ('    spectrogram image name:', src_spectro_data['filenames'][0])\n",
    "print ('    spectrogram image key-mode pair:',\\\n",
    "       src_spectro_data['target_names'][src_spectro_data['target'][0]],\\\n",
    "       '=', src_spectro_data['DESCR'][src_spectro_data['target'][0]],\\\n",
    "       '= target class', src_spectro_data['target'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in images, convert to tensors**\n",
    "\n",
    "Keras Conv2D layers expect a **4D tensor with shape (batch, rows, cols, channels)** (if param data_format='channels_last') (src: <a href='https://keras.io/layers/convolutional/#conv2d'>Keras Conv2D</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] image size: (150, 128)\n",
      "[i] pixel format: RGB\n"
     ]
    }
   ],
   "source": [
    "# open a random image and take a look at the attributes\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open (src_spectro_data['filenames'][0])\n",
    "print ('[i] image size:', im.size)\n",
    "print ('[i] pixel format:', im.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing target size of image**\n",
    "\n",
    "CNNs work best if input size is divisible by 2 many times - image size needs to be changed. (<a href='http://cs231n.github.io/convolutional-networks/#layersizepat'>cs231n - Layer Sizing Patterns</a>)\n",
    "\n",
    "Current image size is 150 x 128: possible options\n",
    "- (-) cut down the image to 128 x 128: information loss in song length\n",
    "- (-) resize to 150 x 150: not divisible by 2 many times (exactly 1 time)\n",
    "- (+) resize to 160 x 160: divisible by 2 many times (exactly 5 times, this is enough)\n",
    "\n",
    "Resizing is done by appending zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PARAM_TARGET_SIZE = 160\n",
    "def resize_image (img_arr):\n",
    "    #print ('>>> resizing image to [{}, {}]...'.format (PARAM_IMG_SIZE, PARAM_IMG_SIZE), end=' ', flush=True)\n",
    "    \n",
    "    m = img_arr.shape[0]\n",
    "    n = img_arr.shape[1]\n",
    "    \n",
    "    # how many additional cols to add?\n",
    "    cols_to_add = PARAM_TARGET_SIZE - n\n",
    "    \n",
    "    img_resized = np.empty ((1, PARAM_TARGET_SIZE))\n",
    "    for i in range (m):\n",
    "        new_line = np.append (img_arr[i], np.zeros (cols_to_add))\n",
    "        img_resized = np.vstack ((img_resized, new_line))\n",
    "    \n",
    "    img_resized = img_resized[1:]\n",
    "    # now img_resized = (128, 160)\n",
    "    \n",
    "    # how many additional rows to add?\n",
    "    rows_to_add = PARAM_TARGET_SIZE - m\n",
    "    img_resized = np.vstack ((img_resized, np.zeros ((rows_to_add, PARAM_TARGET_SIZE))))\n",
    "    # now img_resize = (160, 160)\n",
    "    \n",
    "    #print ('done')\n",
    "    \n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling by feature standardization**\n",
    "\n",
    "$x^{'}= \\frac{x-\\bar{x}}{\\sigma}$\n",
    "\n",
    "tensorflow function used to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below functions read the images and convert those to tensors - original code taken from Udacity MLND dog-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "PARAM_USE_IMG_STD = True\n",
    "\n",
    "def path_to_tensor (img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img (img_path, color_mode='grayscale')\n",
    "    \n",
    "    # convert PIL.Image.Image type to 3D tensor\n",
    "    x = image.img_to_array (img)\n",
    "    x = resize_image (x)\n",
    "    x = x[:,:,np.newaxis]\n",
    "    \n",
    "    # feature standardization to zero mean and stdev of one\n",
    "    # [2018-09-04] turned out that standardization prevents good learning progress\n",
    "    # [2018-09-06] try again with standardization\n",
    "    if PARAM_USE_IMG_STD:\n",
    "        x = K.eval (tf.image.per_image_standardization (x))\n",
    "    \n",
    "    # convert 3D tensor to 4D tensor\n",
    "    return np.expand_dims (x, axis=0)\n",
    "\n",
    "def paths_to_tensor (img_paths):\n",
    "    list_of_tensors = [path_to_tensor (img_path) for img_path in tqdm (img_paths)]\n",
    "    return np.vstack (list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 936/936 [02:07<00:00,  7.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "spectro_tensors = paths_to_tensor (src_spectro_data['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] shape of spectrogram tensors: (936, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('[i] shape of spectrogram tensors:', spectro_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of output classes: 24\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "targets = np_utils.to_categorical (np.array (src_spectro_data['target']), 24)\n",
    "print ('[i] number of output classes:', targets.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test set\n",
    "[2018-09-03] obsolete since test data is in separate directory now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Training dataset consists of 936 samples\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = \\\n",
    "#    train_test_split (spectro_tensors, targets, test_size=(8/39), shuffle=True, random_state=PARAM_RND_STATE)\n",
    "\n",
    "#print ('[i] Training dataset consists of {} samples'.format (X_train.shape[0]))\n",
    "#print ('[i] Testing dataset consists of {} samples'.format (X_test.shape[0]))\n",
    "\n",
    "X_train = spectro_tensors\n",
    "y_train = targets\n",
    "print ('[i] Training dataset consists of {} samples'.format (X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "(<a href='http://cs231n.github.io/convolutional-networks/#layersizepat'>cs231n - Layer Sizing Patterns</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 64)      320       \n",
      "_________________________________________________________________\n",
      "maxp_1 (MaxPooling2D)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 80, 80, 128)       32896     \n",
      "_________________________________________________________________\n",
      "maxp_2 (MaxPooling2D)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 40, 256)       131328    \n",
      "_________________________________________________________________\n",
      "maxp_3 (MaxPooling2D)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "avg_flat (GlobalAveragePooli (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 24)                6168      \n",
      "=================================================================\n",
      "Total params: 170,712\n",
      "Trainable params: 170,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# clear everything known of past instances (\"useful to avoid clutter from old models / layers\")\n",
    "K.clear_session ()\n",
    "\n",
    "# input layer\n",
    "inputs = layers.Input (shape=spectro_tensors.shape[1:], name='input')\n",
    "\n",
    "# hidden layers\n",
    "net = layers.Conv2D (filters=64, kernel_size=(2,2), strides=(1,1),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     name='conv2d_1') (inputs)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_1') (net)\n",
    "\n",
    "net = layers.Conv2D (filters=128, kernel_size=(2,2), strides=(1,1),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     name='conv2d_2') (net)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_2') (net)\n",
    "\n",
    "net = layers.Conv2D (filters=256, kernel_size=(2,2), strides=(1,1),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     name='conv2d_3') (net)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_3') (net)\n",
    "\n",
    "\n",
    "# 'flatten layer'\n",
    "net = layers.GlobalAveragePooling2D (name='avg_flat') (net)\n",
    "\n",
    "net = layers.Dropout (0.25, seed=PARAM_RND_STATE) (net)\n",
    "\n",
    "# output layer\n",
    "outputs = layers.Dense (units=targets.shape[1], activation='softmax', name='output') (net)\n",
    "\n",
    "\n",
    "model = models.Model (inputs=inputs, outputs=outputs)\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter\n",
    "(metric, loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: Arseny Kravchenko http://arseny.info/2017/f-beta-score-for-keras.html\n",
    "from keras import backend as K\n",
    "\n",
    "PARAM_BETA = 1\n",
    "def fbeta (y_true, y_pred):\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip (y_pred, 0, 1)\n",
    "\n",
    "    tp = K.sum (K.round (y_true * y_pred)) + K.epsilon ()\n",
    "    fp = K.sum (K.round (K.clip (y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum (K.round (K.clip (y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = PARAM_BETA ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, losses\n",
    "\n",
    "# old params: lr=0.001, mom=0.8\n",
    "PARAM_LR = 0.1 #0.0005\n",
    "PARAM_MOM = 0.3 #0.95\n",
    "opt = optimizers.SGD (lr=PARAM_LR, momentum=PARAM_MOM)\n",
    "\n",
    "loss = losses.categorical_crossentropy\n",
    "\n",
    "model.compile (optimizer=opt, loss=loss, metrics=[fbeta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 144 samples\n",
      "Epoch 1/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 3.1907 - fbeta: 4.5455e-09 - val_loss: 3.1837 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.18374, saving model to model/model.w.best.h5\n",
      "Epoch 2/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 3.1859 - fbeta: 4.5455e-09 - val_loss: 3.1852 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.18374\n",
      "Epoch 3/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 3.1769 - fbeta: 4.5455e-09 - val_loss: 3.1854 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.18374\n",
      "Epoch 4/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 3.1777 - fbeta: 4.5455e-09 - val_loss: 3.1841 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.18374\n",
      "Epoch 5/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1754 - fbeta: 4.5455e-09 - val_loss: 3.1821 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.18374 to 3.18214, saving model to model/model.w.best.h5\n",
      "Epoch 6/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1804 - fbeta: 4.5455e-09 - val_loss: 3.1820 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.18214 to 3.18202, saving model to model/model.w.best.h5\n",
      "Epoch 7/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1694 - fbeta: 4.5455e-09 - val_loss: 3.1803 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.18202 to 3.18029, saving model to model/model.w.best.h5\n",
      "Epoch 8/450\n",
      "792/792 [==============================] - 76s 96ms/step - loss: 3.1720 - fbeta: 4.5455e-09 - val_loss: 3.1768 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.18029 to 3.17680, saving model to model/model.w.best.h5\n",
      "Epoch 9/450\n",
      "792/792 [==============================] - 75s 94ms/step - loss: 3.1680 - fbeta: 4.5455e-09 - val_loss: 3.1759 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.17680 to 3.17585, saving model to model/model.w.best.h5\n",
      "Epoch 10/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1625 - fbeta: 4.5455e-09 - val_loss: 3.1743 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.17585 to 3.17434, saving model to model/model.w.best.h5\n",
      "Epoch 11/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 3.1570 - fbeta: 4.5455e-09 - val_loss: 3.1671 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.17434 to 3.16706, saving model to model/model.w.best.h5\n",
      "Epoch 12/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 3.1496 - fbeta: 4.5455e-09 - val_loss: 3.1705 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.16706\n",
      "Epoch 13/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1416 - fbeta: 4.5455e-09 - val_loss: 3.1585 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.16706 to 3.15850, saving model to model/model.w.best.h5\n",
      "Epoch 14/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1414 - fbeta: 4.5455e-09 - val_loss: 3.1482 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.15850 to 3.14818, saving model to model/model.w.best.h5\n",
      "Epoch 15/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1325 - fbeta: 4.5455e-09 - val_loss: 3.1222 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.14818 to 3.12218, saving model to model/model.w.best.h5\n",
      "Epoch 16/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.1167 - fbeta: 4.5455e-09 - val_loss: 3.1092 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.12218 to 3.10922, saving model to model/model.w.best.h5\n",
      "Epoch 17/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.0902 - fbeta: 4.5455e-09 - val_loss: 3.0844 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.10922 to 3.08439, saving model to model/model.w.best.h5\n",
      "Epoch 18/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 3.0561 - fbeta: 4.5455e-09 - val_loss: 3.0382 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.08439 to 3.03816, saving model to model/model.w.best.h5\n",
      "Epoch 19/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 2.9991 - fbeta: 4.5455e-09 - val_loss: 3.0057 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.03816 to 3.00565, saving model to model/model.w.best.h5\n",
      "Epoch 20/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.9547 - fbeta: 4.5455e-09 - val_loss: 2.9445 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.00565 to 2.94446, saving model to model/model.w.best.h5\n",
      "Epoch 21/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.8905 - fbeta: 4.5455e-09 - val_loss: 2.8847 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.94446 to 2.88467, saving model to model/model.w.best.h5\n",
      "Epoch 22/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.8553 - fbeta: 0.0025 - val_loss: 2.8334 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.88467 to 2.83335, saving model to model/model.w.best.h5\n",
      "Epoch 23/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.7796 - fbeta: 4.5455e-09 - val_loss: 2.7566 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.83335 to 2.75661, saving model to model/model.w.best.h5\n",
      "Epoch 24/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.7508 - fbeta: 4.5455e-09 - val_loss: 2.7469 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.75661 to 2.74693, saving model to model/model.w.best.h5\n",
      "Epoch 25/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.7200 - fbeta: 0.0122 - val_loss: 2.7187 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.74693 to 2.71866, saving model to model/model.w.best.h5\n",
      "Epoch 26/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.6545 - fbeta: 0.0171 - val_loss: 2.6588 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.71866 to 2.65885, saving model to model/model.w.best.h5\n",
      "Epoch 27/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.6167 - fbeta: 0.0194 - val_loss: 2.7190 - val_fbeta: 5.4039e-09\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.65885\n",
      "Epoch 28/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.6224 - fbeta: 0.0291 - val_loss: 2.5896 - val_fbeta: 0.0266\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.65885 to 2.58960, saving model to model/model.w.best.h5\n",
      "Epoch 29/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.5920 - fbeta: 0.0168 - val_loss: 2.5765 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.58960 to 2.57649, saving model to model/model.w.best.h5\n",
      "Epoch 30/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 2.5315 - fbeta: 0.0217 - val_loss: 2.6008 - val_fbeta: 0.0136\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.57649\n",
      "Epoch 31/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.5274 - fbeta: 0.0241 - val_loss: 2.5043 - val_fbeta: 5.5556e-09\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.57649 to 2.50430, saving model to model/model.w.best.h5\n",
      "Epoch 32/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.4820 - fbeta: 0.0332 - val_loss: 2.4879 - val_fbeta: 0.0266\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.50430 to 2.48787, saving model to model/model.w.best.h5\n",
      "Epoch 33/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.4961 - fbeta: 0.0265 - val_loss: 2.5881 - val_fbeta: 0.0136\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.48787\n",
      "Epoch 34/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.4554 - fbeta: 0.0485 - val_loss: 2.4906 - val_fbeta: 0.0269\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.48787\n",
      "Epoch 35/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.4413 - fbeta: 0.0508 - val_loss: 2.5826 - val_fbeta: 0.0379\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.48787\n",
      "Epoch 36/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.4659 - fbeta: 0.0475 - val_loss: 2.4811 - val_fbeta: 0.0260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss improved from 2.48787 to 2.48106, saving model to model/model.w.best.h5\n",
      "Epoch 37/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.4194 - fbeta: 0.0664 - val_loss: 2.4420 - val_fbeta: 0.0266\n",
      "\n",
      "Epoch 00037: val_loss improved from 2.48106 to 2.44205, saving model to model/model.w.best.h5\n",
      "Epoch 38/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3825 - fbeta: 0.0646 - val_loss: 2.5404 - val_fbeta: 0.0729\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.44205\n",
      "Epoch 39/450\n",
      "792/792 [==============================] - 75s 95ms/step - loss: 2.4284 - fbeta: 0.0745 - val_loss: 2.4941 - val_fbeta: 0.0520\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.44205\n",
      "Epoch 40/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.4330 - fbeta: 0.0603 - val_loss: 2.4510 - val_fbeta: 0.0382\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.44205\n",
      "Epoch 41/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3592 - fbeta: 0.0642 - val_loss: 2.4990 - val_fbeta: 0.0634\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.44205\n",
      "Epoch 42/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3430 - fbeta: 0.0880 - val_loss: 2.5701 - val_fbeta: 0.0393\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.44205\n",
      "Epoch 43/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3758 - fbeta: 0.0893 - val_loss: 2.4619 - val_fbeta: 0.0266\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.44205\n",
      "Epoch 44/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3496 - fbeta: 0.1045 - val_loss: 2.4742 - val_fbeta: 0.0509\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.44205\n",
      "Epoch 45/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3744 - fbeta: 0.0778 - val_loss: 2.4767 - val_fbeta: 0.0744\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.44205\n",
      "Epoch 46/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3146 - fbeta: 0.0836 - val_loss: 2.4406 - val_fbeta: 0.0645\n",
      "\n",
      "Epoch 00046: val_loss improved from 2.44205 to 2.44063, saving model to model/model.w.best.h5\n",
      "Epoch 47/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3255 - fbeta: 0.0861 - val_loss: 2.3683 - val_fbeta: 0.0905\n",
      "\n",
      "Epoch 00047: val_loss improved from 2.44063 to 2.36831, saving model to model/model.w.best.h5\n",
      "Epoch 48/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2903 - fbeta: 0.1083 - val_loss: 2.4671 - val_fbeta: 0.0523\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.36831\n",
      "Epoch 49/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.3295 - fbeta: 0.0817 - val_loss: 2.4290 - val_fbeta: 0.0892\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.36831\n",
      "Epoch 50/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.3017 - fbeta: 0.1175 - val_loss: 2.5751 - val_fbeta: 0.0492\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.36831\n",
      "Epoch 51/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3123 - fbeta: 0.1072 - val_loss: 2.4383 - val_fbeta: 0.0757\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.36831\n",
      "Epoch 52/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2460 - fbeta: 0.1051 - val_loss: 2.3629 - val_fbeta: 0.0764\n",
      "\n",
      "Epoch 00052: val_loss improved from 2.36831 to 2.36292, saving model to model/model.w.best.h5\n",
      "Epoch 53/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2664 - fbeta: 0.1250 - val_loss: 2.3897 - val_fbeta: 0.0626\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.36292\n",
      "Epoch 54/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2459 - fbeta: 0.1249 - val_loss: 2.4149 - val_fbeta: 0.0130\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.36292\n",
      "Epoch 55/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.3090 - fbeta: 0.1136 - val_loss: 2.3142 - val_fbeta: 0.0510\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.36292 to 2.31415, saving model to model/model.w.best.h5\n",
      "Epoch 56/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2711 - fbeta: 0.1192 - val_loss: 2.4777 - val_fbeta: 0.0706\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.31415\n",
      "Epoch 57/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2361 - fbeta: 0.1385 - val_loss: 2.4966 - val_fbeta: 0.1002\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.31415\n",
      "Epoch 58/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2278 - fbeta: 0.1637 - val_loss: 2.3769 - val_fbeta: 0.0510\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.31415\n",
      "Epoch 59/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2241 - fbeta: 0.1488 - val_loss: 2.3726 - val_fbeta: 0.1235\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.31415\n",
      "Epoch 60/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2767 - fbeta: 0.1698 - val_loss: 2.3513 - val_fbeta: 0.0879\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.31415\n",
      "Epoch 61/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.2055 - fbeta: 0.1243 - val_loss: 2.3989 - val_fbeta: 0.0806\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.31415\n",
      "Epoch 62/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2457 - fbeta: 0.1454 - val_loss: 2.3339 - val_fbeta: 0.1008\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.31415\n",
      "Epoch 63/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.2020 - fbeta: 0.1623 - val_loss: 2.5965 - val_fbeta: 0.1016\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.31415\n",
      "Epoch 64/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.1957 - fbeta: 0.1633 - val_loss: 2.2786 - val_fbeta: 0.1392\n",
      "\n",
      "Epoch 00064: val_loss improved from 2.31415 to 2.27857, saving model to model/model.w.best.h5\n",
      "Epoch 65/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.1564 - fbeta: 0.2042 - val_loss: 2.2843 - val_fbeta: 0.1131\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.27857\n",
      "Epoch 66/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.1632 - fbeta: 0.1715 - val_loss: 2.4762 - val_fbeta: 0.1166\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.27857\n",
      "Epoch 67/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.1674 - fbeta: 0.1923 - val_loss: 2.4021 - val_fbeta: 0.1618\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.27857\n",
      "Epoch 68/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 2.1972 - fbeta: 0.1810 - val_loss: 2.3112 - val_fbeta: 0.1349\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.27857\n",
      "Epoch 69/450\n",
      "792/792 [==============================] - 72s 91ms/step - loss: 2.1505 - fbeta: 0.1799 - val_loss: 2.3492 - val_fbeta: 0.1401\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.27857\n",
      "Epoch 70/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.1539 - fbeta: 0.1872 - val_loss: 2.2852 - val_fbeta: 0.1092\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.27857\n",
      "Epoch 71/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.1458 - fbeta: 0.1880 - val_loss: 2.3436 - val_fbeta: 0.1965\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.27857\n",
      "Epoch 72/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 2.2102 - fbeta: 0.1881 - val_loss: 2.4273 - val_fbeta: 0.1517\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.27857\n",
      "Epoch 73/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.1701 - fbeta: 0.1885 - val_loss: 2.4276 - val_fbeta: 0.1924\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.27857\n",
      "Epoch 74/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0801 - fbeta: 0.2089 - val_loss: 2.1697 - val_fbeta: 0.0963\n",
      "\n",
      "Epoch 00074: val_loss improved from 2.27857 to 2.16966, saving model to model/model.w.best.h5\n",
      "Epoch 75/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0847 - fbeta: 0.2127 - val_loss: 2.2394 - val_fbeta: 0.0997\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.16966\n",
      "Epoch 76/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.0894 - fbeta: 0.1926 - val_loss: 2.1866 - val_fbeta: 0.1438\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.16966\n",
      "Epoch 77/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.1242 - fbeta: 0.2128 - val_loss: 2.2572 - val_fbeta: 0.0877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00077: val_loss did not improve from 2.16966\n",
      "Epoch 78/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.0396 - fbeta: 0.1840 - val_loss: 2.3025 - val_fbeta: 0.1187\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.16966\n",
      "Epoch 79/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.0913 - fbeta: 0.2212 - val_loss: 2.2016 - val_fbeta: 0.1213\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.16966\n",
      "Epoch 80/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0656 - fbeta: 0.2521 - val_loss: 2.2117 - val_fbeta: 0.0660\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.16966\n",
      "Epoch 81/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.0193 - fbeta: 0.2435 - val_loss: 2.3121 - val_fbeta: 0.1542\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.16966\n",
      "Epoch 82/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0932 - fbeta: 0.2436 - val_loss: 2.4303 - val_fbeta: 0.2114\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.16966\n",
      "Epoch 83/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0514 - fbeta: 0.2453 - val_loss: 2.2144 - val_fbeta: 0.1579\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.16966\n",
      "Epoch 84/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 2.0130 - fbeta: 0.2711 - val_loss: 2.1930 - val_fbeta: 0.2108\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.16966\n",
      "Epoch 85/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.0913 - fbeta: 0.2519 - val_loss: 2.1290 - val_fbeta: 0.1662\n",
      "\n",
      "Epoch 00085: val_loss improved from 2.16966 to 2.12903, saving model to model/model.w.best.h5\n",
      "Epoch 86/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.9902 - fbeta: 0.2756 - val_loss: 2.4635 - val_fbeta: 0.1031\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.12903\n",
      "Epoch 87/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9732 - fbeta: 0.2515 - val_loss: 2.4524 - val_fbeta: 0.1690\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.12903\n",
      "Epoch 88/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.9542 - fbeta: 0.2662 - val_loss: 2.1883 - val_fbeta: 0.1258\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.12903\n",
      "Epoch 89/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.9102 - fbeta: 0.3196 - val_loss: 2.0904 - val_fbeta: 0.1486\n",
      "\n",
      "Epoch 00089: val_loss improved from 2.12903 to 2.09043, saving model to model/model.w.best.h5\n",
      "Epoch 90/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8799 - fbeta: 0.2929 - val_loss: 2.0912 - val_fbeta: 0.1610\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.09043\n",
      "Epoch 91/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.9440 - fbeta: 0.2559 - val_loss: 2.0935 - val_fbeta: 0.1457\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.09043\n",
      "Epoch 92/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9827 - fbeta: 0.2506 - val_loss: 2.4854 - val_fbeta: 0.1457\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.09043\n",
      "Epoch 93/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9576 - fbeta: 0.2834 - val_loss: 2.4738 - val_fbeta: 0.1567\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.09043\n",
      "Epoch 94/450\n",
      "792/792 [==============================] - 75s 94ms/step - loss: 1.9589 - fbeta: 0.3048 - val_loss: 2.2267 - val_fbeta: 0.1531\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.09043\n",
      "Epoch 95/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.9590 - fbeta: 0.2993 - val_loss: 2.1898 - val_fbeta: 0.1565\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.09043\n",
      "Epoch 96/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.0439 - fbeta: 0.2937 - val_loss: 2.2298 - val_fbeta: 0.2053\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.09043\n",
      "Epoch 97/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.8310 - fbeta: 0.3261 - val_loss: 2.1234 - val_fbeta: 0.1867\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.09043\n",
      "Epoch 98/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9605 - fbeta: 0.2823 - val_loss: 2.0836 - val_fbeta: 0.1440\n",
      "\n",
      "Epoch 00098: val_loss improved from 2.09043 to 2.08362, saving model to model/model.w.best.h5\n",
      "Epoch 99/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9239 - fbeta: 0.3033 - val_loss: 2.1455 - val_fbeta: 0.1686\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.08362\n",
      "Epoch 100/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8572 - fbeta: 0.3465 - val_loss: 2.3292 - val_fbeta: 0.1899\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.08362\n",
      "Epoch 101/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9250 - fbeta: 0.3236 - val_loss: 2.1528 - val_fbeta: 0.1555\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2.08362\n",
      "Epoch 102/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8716 - fbeta: 0.3450 - val_loss: 2.1075 - val_fbeta: 0.2477\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2.08362\n",
      "Epoch 103/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8942 - fbeta: 0.3421 - val_loss: 2.0353 - val_fbeta: 0.2499\n",
      "\n",
      "Epoch 00103: val_loss improved from 2.08362 to 2.03527, saving model to model/model.w.best.h5\n",
      "Epoch 104/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8507 - fbeta: 0.3260 - val_loss: 2.0250 - val_fbeta: 0.2205\n",
      "\n",
      "Epoch 00104: val_loss improved from 2.03527 to 2.02502, saving model to model/model.w.best.h5\n",
      "Epoch 105/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9089 - fbeta: 0.3659 - val_loss: 2.1844 - val_fbeta: 0.1681\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 2.02502\n",
      "Epoch 106/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8538 - fbeta: 0.3474 - val_loss: 2.0233 - val_fbeta: 0.2505\n",
      "\n",
      "Epoch 00106: val_loss improved from 2.02502 to 2.02330, saving model to model/model.w.best.h5\n",
      "Epoch 107/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9424 - fbeta: 0.3793 - val_loss: 2.1300 - val_fbeta: 0.1894\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2.02330\n",
      "Epoch 108/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8832 - fbeta: 0.3259 - val_loss: 2.0692 - val_fbeta: 0.2066\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 2.02330\n",
      "Epoch 109/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8573 - fbeta: 0.3379 - val_loss: 1.9483 - val_fbeta: 0.2322\n",
      "\n",
      "Epoch 00109: val_loss improved from 2.02330 to 1.94833, saving model to model/model.w.best.h5\n",
      "Epoch 110/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.9463 - fbeta: 0.3530 - val_loss: 2.3485 - val_fbeta: 0.1797\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.94833\n",
      "Epoch 111/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7653 - fbeta: 0.3884 - val_loss: 2.6936 - val_fbeta: 0.1617\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.94833\n",
      "Epoch 112/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 2.0648 - fbeta: 0.3663 - val_loss: 2.2131 - val_fbeta: 0.2263\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.94833\n",
      "Epoch 113/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7682 - fbeta: 0.4011 - val_loss: 2.1845 - val_fbeta: 0.2396\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.94833\n",
      "Epoch 114/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8428 - fbeta: 0.3659 - val_loss: 2.3281 - val_fbeta: 0.1983\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.94833\n",
      "Epoch 115/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7575 - fbeta: 0.3694 - val_loss: 2.1044 - val_fbeta: 0.2463\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.94833\n",
      "Epoch 116/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7766 - fbeta: 0.3689 - val_loss: 2.0321 - val_fbeta: 0.2786\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.94833\n",
      "Epoch 117/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.7482 - fbeta: 0.3777 - val_loss: 2.0462 - val_fbeta: 0.2407\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.94833\n",
      "Epoch 118/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7931 - fbeta: 0.3725 - val_loss: 2.0269 - val_fbeta: 0.2729\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.94833\n",
      "Epoch 119/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 75s 94ms/step - loss: 1.7436 - fbeta: 0.3924 - val_loss: 2.1462 - val_fbeta: 0.2778\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.94833\n",
      "Epoch 120/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.6234 - fbeta: 0.4377 - val_loss: 2.0143 - val_fbeta: 0.3007\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.94833\n",
      "Epoch 121/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.7677 - fbeta: 0.3875 - val_loss: 1.9820 - val_fbeta: 0.2973\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.94833\n",
      "Epoch 122/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.7109 - fbeta: 0.4126 - val_loss: 1.9363 - val_fbeta: 0.2732\n",
      "\n",
      "Epoch 00122: val_loss improved from 1.94833 to 1.93632, saving model to model/model.w.best.h5\n",
      "Epoch 123/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.7216 - fbeta: 0.3966 - val_loss: 1.9460 - val_fbeta: 0.2861\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.93632\n",
      "Epoch 124/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8749 - fbeta: 0.4035 - val_loss: 2.2976 - val_fbeta: 0.1978\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.93632\n",
      "Epoch 125/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.6518 - fbeta: 0.4466 - val_loss: 1.9901 - val_fbeta: 0.2564\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.93632\n",
      "Epoch 126/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.7059 - fbeta: 0.4019 - val_loss: 2.4101 - val_fbeta: 0.3023\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.93632\n",
      "Epoch 127/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.9372 - fbeta: 0.3897 - val_loss: 2.3488 - val_fbeta: 0.2494\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.93632\n",
      "Epoch 128/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.6342 - fbeta: 0.4413 - val_loss: 2.3688 - val_fbeta: 0.2933\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.93632\n",
      "Epoch 129/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.5206 - fbeta: 0.4646 - val_loss: 1.7572 - val_fbeta: 0.3374\n",
      "\n",
      "Epoch 00129: val_loss improved from 1.93632 to 1.75715, saving model to model/model.w.best.h5\n",
      "Epoch 130/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.5057 - fbeta: 0.4938 - val_loss: 1.8099 - val_fbeta: 0.3662\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.75715\n",
      "Epoch 131/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.9162 - fbeta: 0.4357 - val_loss: 4.6004 - val_fbeta: 0.2906\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.75715\n",
      "Epoch 132/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 2.2690 - fbeta: 0.4248 - val_loss: 1.8101 - val_fbeta: 0.2957\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.75715\n",
      "Epoch 133/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.4367 - fbeta: 0.4881 - val_loss: 1.9573 - val_fbeta: 0.3444\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.75715\n",
      "Epoch 134/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.8535 - fbeta: 0.4545 - val_loss: 2.0325 - val_fbeta: 0.3514\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.75715\n",
      "Epoch 135/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.5744 - fbeta: 0.4902 - val_loss: 1.9229 - val_fbeta: 0.3228\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.75715\n",
      "Epoch 136/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.6673 - fbeta: 0.4819 - val_loss: 2.0791 - val_fbeta: 0.3549\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.75715\n",
      "Epoch 137/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.5511 - fbeta: 0.4628 - val_loss: 1.8088 - val_fbeta: 0.3927\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.75715\n",
      "Epoch 138/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.8660 - fbeta: 0.4269 - val_loss: 1.9140 - val_fbeta: 0.2957\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.75715\n",
      "Epoch 139/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.4657 - fbeta: 0.5238 - val_loss: 1.8534 - val_fbeta: 0.3820\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.75715\n",
      "Epoch 140/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.4755 - fbeta: 0.5110 - val_loss: 1.8456 - val_fbeta: 0.3976\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.75715\n",
      "Epoch 141/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.7949 - fbeta: 0.4724 - val_loss: 1.9576 - val_fbeta: 0.3780\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.75715\n",
      "Epoch 142/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.6018 - fbeta: 0.5026 - val_loss: 1.6878 - val_fbeta: 0.4118\n",
      "\n",
      "Epoch 00142: val_loss improved from 1.75715 to 1.68777, saving model to model/model.w.best.h5\n",
      "Epoch 143/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.2654 - fbeta: 0.5682 - val_loss: 1.6858 - val_fbeta: 0.4006\n",
      "\n",
      "Epoch 00143: val_loss improved from 1.68777 to 1.68579, saving model to model/model.w.best.h5\n",
      "Epoch 144/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.6297 - fbeta: 0.5042 - val_loss: 1.8160 - val_fbeta: 0.4236\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.68579\n",
      "Epoch 145/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.8738 - fbeta: 0.4605 - val_loss: 2.4035 - val_fbeta: 0.3569\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.68579\n",
      "Epoch 146/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.4546 - fbeta: 0.5372 - val_loss: 2.7627 - val_fbeta: 0.3219\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.68579\n",
      "Epoch 147/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.4698 - fbeta: 0.5392 - val_loss: 1.6799 - val_fbeta: 0.3680\n",
      "\n",
      "Epoch 00147: val_loss improved from 1.68579 to 1.67988, saving model to model/model.w.best.h5\n",
      "Epoch 148/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.6109 - fbeta: 0.5137 - val_loss: 3.1356 - val_fbeta: 0.3196\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.67988\n",
      "Epoch 149/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.4686 - fbeta: 0.5473 - val_loss: 1.9964 - val_fbeta: 0.4365\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.67988\n",
      "Epoch 150/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.3051 - fbeta: 0.5826 - val_loss: 2.0712 - val_fbeta: 0.3759\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.67988\n",
      "Epoch 151/450\n",
      "792/792 [==============================] - 74s 94ms/step - loss: 1.3908 - fbeta: 0.5375 - val_loss: 1.7344 - val_fbeta: 0.5027\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.67988\n",
      "Epoch 152/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.2036 - fbeta: 0.5664 - val_loss: 1.7511 - val_fbeta: 0.4361\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.67988\n",
      "Epoch 153/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.3984 - fbeta: 0.5651 - val_loss: 1.5999 - val_fbeta: 0.4792\n",
      "\n",
      "Epoch 00153: val_loss improved from 1.67988 to 1.59992, saving model to model/model.w.best.h5\n",
      "Epoch 154/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.2434 - fbeta: 0.6000 - val_loss: 1.4194 - val_fbeta: 0.5314\n",
      "\n",
      "Epoch 00154: val_loss improved from 1.59992 to 1.41938, saving model to model/model.w.best.h5\n",
      "Epoch 155/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.4243 - fbeta: 0.5493 - val_loss: 1.8616 - val_fbeta: 0.4037\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.41938\n",
      "Epoch 156/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 1.2110 - fbeta: 0.5808 - val_loss: 1.8739 - val_fbeta: 0.4932\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.41938\n",
      "Epoch 157/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.1193 - fbeta: 0.6097 - val_loss: 1.3517 - val_fbeta: 0.5365\n",
      "\n",
      "Epoch 00157: val_loss improved from 1.41938 to 1.35169, saving model to model/model.w.best.h5\n",
      "Epoch 158/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.3682 - fbeta: 0.5824 - val_loss: 1.8307 - val_fbeta: 0.4758\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.35169\n",
      "Epoch 159/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.2069 - fbeta: 0.6061 - val_loss: 1.4126 - val_fbeta: 0.5567\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.35169\n",
      "Epoch 160/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 73s 93ms/step - loss: 1.3718 - fbeta: 0.6001 - val_loss: 2.1952 - val_fbeta: 0.5204\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.35169\n",
      "Epoch 161/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 1.3795 - fbeta: 0.5564 - val_loss: 1.4210 - val_fbeta: 0.5259\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.35169\n",
      "Epoch 162/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 1.1899 - fbeta: 0.6069 - val_loss: 3.3514 - val_fbeta: 0.4123\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.35169\n",
      "Epoch 163/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 11.8603 - fbeta: 0.1611 - val_loss: 15.7823 - val_fbeta: 0.0208\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.35169\n",
      "Epoch 164/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.0935 - fbeta: 0.0568 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.35169\n",
      "Epoch 165/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3843 - fbeta: 0.0455 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.35169\n",
      "Epoch 166/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.35169\n",
      "Epoch 167/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.35169\n",
      "Epoch 168/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.35169\n",
      "Epoch 169/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.35169\n",
      "Epoch 170/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.35169\n",
      "Epoch 171/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.35169\n",
      "Epoch 172/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.35169\n",
      "Epoch 173/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.35169\n",
      "Epoch 174/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.35169\n",
      "Epoch 175/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3987 - fbeta: 0.0442 - val_loss: 15.4311 - val_fbeta: 0.0347\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.35169\n",
      "Epoch 176/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.2337 - fbeta: 0.0543 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.35169\n",
      "Epoch 177/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.2788 - fbeta: 0.0518 - val_loss: 15.3346 - val_fbeta: 0.0486\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.35169\n",
      "Epoch 178/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.2839 - fbeta: 0.0505 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.35169\n",
      "Epoch 179/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3041 - fbeta: 0.0505 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.35169\n",
      "Epoch 180/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.1787 - fbeta: 0.0581 - val_loss: 15.5584 - val_fbeta: 0.0347\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.35169\n",
      "Epoch 181/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3421 - fbeta: 0.0480 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.35169\n",
      "Epoch 182/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.3857 - fbeta: 0.0455 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.35169\n",
      "Epoch 183/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.0258 - fbeta: 0.0669 - val_loss: 15.5584 - val_fbeta: 0.0347\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.35169\n",
      "Epoch 184/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.1533 - fbeta: 0.0593 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.35169\n",
      "Epoch 185/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3041 - fbeta: 0.0505 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.35169\n",
      "Epoch 186/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.3306 - fbeta: 0.0480 - val_loss: 14.9988 - val_fbeta: 0.0694\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.35169\n",
      "Epoch 187/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.2488 - fbeta: 0.0530 - val_loss: 15.4465 - val_fbeta: 0.0417\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.35169\n",
      "Epoch 188/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.2220 - fbeta: 0.0556 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.35169\n",
      "Epoch 189/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.1591 - fbeta: 0.0593 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.35169\n",
      "Epoch 190/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.35169\n",
      "Epoch 191/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.35169\n",
      "Epoch 192/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.35169\n",
      "Epoch 193/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.35169\n",
      "Epoch 194/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.35169\n",
      "Epoch 195/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.35169\n",
      "Epoch 196/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.35169\n",
      "Epoch 197/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.35169\n",
      "Epoch 198/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.35169\n",
      "Epoch 199/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.35169\n",
      "Epoch 200/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.35169\n",
      "Epoch 201/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.35169\n",
      "Epoch 202/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.35169\n",
      "Epoch 203/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.35169\n",
      "Epoch 204/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.35169\n",
      "Epoch 205/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.35169\n",
      "Epoch 206/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.35169\n",
      "Epoch 207/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.35169\n",
      "Epoch 208/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.35169\n",
      "Epoch 209/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.35169\n",
      "Epoch 210/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.35169\n",
      "Epoch 211/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.35169\n",
      "Epoch 212/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.35169\n",
      "Epoch 213/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.35169\n",
      "Epoch 214/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.35169\n",
      "Epoch 215/450\n",
      "792/792 [==============================] - 73s 92ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.35169\n",
      "Epoch 216/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.35169\n",
      "Epoch 217/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.35169\n",
      "Epoch 218/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.35169\n",
      "Epoch 219/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.35169\n",
      "Epoch 220/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.35169\n",
      "Epoch 221/450\n",
      "792/792 [==============================] - 73s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.35169\n",
      "Epoch 222/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.35169\n",
      "Epoch 223/450\n",
      "792/792 [==============================] - 74s 93ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.35169\n",
      "Epoch 224/450\n",
      "792/792 [==============================] - 71s 90ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.35169\n",
      "Epoch 225/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.35169\n",
      "Epoch 226/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.35169\n",
      "Epoch 227/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.35169\n",
      "Epoch 228/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.35169\n",
      "Epoch 229/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.35169\n",
      "Epoch 230/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.35169\n",
      "Epoch 231/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.35169\n",
      "Epoch 232/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.35169\n",
      "Epoch 233/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.35169\n",
      "Epoch 234/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.35169\n",
      "Epoch 235/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.35169\n",
      "Epoch 236/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.35169\n",
      "Epoch 237/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.35169\n",
      "Epoch 238/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.35169\n",
      "Epoch 239/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.35169\n",
      "Epoch 240/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.35169\n",
      "Epoch 241/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.35169\n",
      "Epoch 242/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.35169\n",
      "Epoch 243/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.35169\n",
      "Epoch 244/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.35169\n",
      "Epoch 245/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.35169\n",
      "Epoch 246/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.35169\n",
      "Epoch 247/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.35169\n",
      "Epoch 248/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.35169\n",
      "Epoch 249/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.35169\n",
      "Epoch 250/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.35169\n",
      "Epoch 251/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.35169\n",
      "Epoch 252/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.35169\n",
      "Epoch 253/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.35169\n",
      "Epoch 254/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.35169\n",
      "Epoch 255/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.35169\n",
      "Epoch 256/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.35169\n",
      "Epoch 257/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.35169\n",
      "Epoch 258/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.35169\n",
      "Epoch 259/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.35169\n",
      "Epoch 260/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.35169\n",
      "Epoch 261/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.35169\n",
      "Epoch 262/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.35169\n",
      "Epoch 263/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.35169\n",
      "Epoch 264/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.35169\n",
      "Epoch 265/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.35169\n",
      "Epoch 266/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.35169\n",
      "Epoch 267/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.35169\n",
      "Epoch 268/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.35169\n",
      "Epoch 269/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.35169\n",
      "Epoch 270/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.35169\n",
      "Epoch 271/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.35169\n",
      "Epoch 272/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.35169\n",
      "Epoch 273/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.35169\n",
      "Epoch 274/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.35169\n",
      "Epoch 275/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.35169\n",
      "Epoch 276/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.35169\n",
      "Epoch 277/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.35169\n",
      "Epoch 278/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.35169\n",
      "Epoch 279/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.35169\n",
      "Epoch 280/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.35169\n",
      "Epoch 281/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.35169\n",
      "Epoch 282/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.35169\n",
      "Epoch 283/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.35169\n",
      "Epoch 284/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.35169\n",
      "Epoch 285/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.35169\n",
      "Epoch 286/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.35169\n",
      "Epoch 287/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.35169\n",
      "Epoch 288/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.35169\n",
      "Epoch 289/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.35169\n",
      "Epoch 290/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.35169\n",
      "Epoch 291/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.35169\n",
      "Epoch 292/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.35169\n",
      "Epoch 293/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.35169\n",
      "Epoch 294/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.35169\n",
      "Epoch 295/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.35169\n",
      "Epoch 296/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 1.35169\n",
      "Epoch 297/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.35169\n",
      "Epoch 298/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.35169\n",
      "Epoch 299/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.35169\n",
      "Epoch 300/450\n",
      "792/792 [==============================] - 71s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.35169\n",
      "Epoch 301/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.35169\n",
      "Epoch 302/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.35169\n",
      "Epoch 303/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.35169\n",
      "Epoch 304/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.35169\n",
      "Epoch 305/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.35169\n",
      "Epoch 306/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.35169\n",
      "Epoch 307/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.35169\n",
      "Epoch 308/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.35169\n",
      "Epoch 309/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.35169\n",
      "Epoch 310/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.35169\n",
      "Epoch 311/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.35169\n",
      "Epoch 312/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.35169\n",
      "Epoch 313/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.35169\n",
      "Epoch 314/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.35169\n",
      "Epoch 315/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.35169\n",
      "Epoch 316/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.35169\n",
      "Epoch 317/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.35169\n",
      "Epoch 318/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.35169\n",
      "Epoch 319/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.35169\n",
      "Epoch 320/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.35169\n",
      "Epoch 321/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 1.35169\n",
      "Epoch 322/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.35169\n",
      "Epoch 323/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.35169\n",
      "Epoch 324/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 1.35169\n",
      "Epoch 325/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.35169\n",
      "Epoch 326/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.35169\n",
      "Epoch 327/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.35169\n",
      "Epoch 328/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.35169\n",
      "Epoch 329/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.35169\n",
      "Epoch 330/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.35169\n",
      "Epoch 331/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 1.35169\n",
      "Epoch 332/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.35169\n",
      "Epoch 333/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.35169\n",
      "Epoch 334/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.35169\n",
      "Epoch 335/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.35169\n",
      "Epoch 336/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.35169\n",
      "Epoch 337/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.35169\n",
      "Epoch 338/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.35169\n",
      "Epoch 339/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.35169\n",
      "Epoch 340/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 1.35169\n",
      "Epoch 341/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 1.35169\n",
      "Epoch 342/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.35169\n",
      "Epoch 343/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.35169\n",
      "Epoch 344/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 1.35169\n",
      "Epoch 345/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.35169\n",
      "Epoch 346/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.35169\n",
      "Epoch 347/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.35169\n",
      "Epoch 348/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.35169\n",
      "Epoch 349/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.35169\n",
      "Epoch 350/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.35169\n",
      "Epoch 351/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.35169\n",
      "Epoch 352/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.35169\n",
      "Epoch 353/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.35169\n",
      "Epoch 354/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.35169\n",
      "Epoch 355/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.35169\n",
      "Epoch 356/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.35169\n",
      "Epoch 357/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.35169\n",
      "Epoch 358/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.35169\n",
      "Epoch 359/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.35169\n",
      "Epoch 360/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.35169\n",
      "Epoch 361/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.35169\n",
      "Epoch 362/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.35169\n",
      "Epoch 363/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.35169\n",
      "Epoch 364/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.35169\n",
      "Epoch 365/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.35169\n",
      "Epoch 366/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.35169\n",
      "Epoch 367/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.35169\n",
      "Epoch 368/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.35169\n",
      "Epoch 369/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.35169\n",
      "Epoch 370/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.35169\n",
      "Epoch 371/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.35169\n",
      "Epoch 372/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.35169\n",
      "Epoch 373/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.35169\n",
      "Epoch 374/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.35169\n",
      "Epoch 375/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.35169\n",
      "Epoch 376/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.35169\n",
      "Epoch 377/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.35169\n",
      "Epoch 378/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.35169\n",
      "Epoch 379/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.35169\n",
      "Epoch 380/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.35169\n",
      "Epoch 381/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.35169\n",
      "Epoch 382/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.35169\n",
      "Epoch 383/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.35169\n",
      "Epoch 384/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.35169\n",
      "Epoch 385/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.35169\n",
      "Epoch 386/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.35169\n",
      "Epoch 387/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.35169\n",
      "Epoch 388/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.35169\n",
      "Epoch 389/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.35169\n",
      "Epoch 390/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.35169\n",
      "Epoch 391/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.35169\n",
      "Epoch 392/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.35169\n",
      "Epoch 393/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.35169\n",
      "Epoch 394/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.35169\n",
      "Epoch 395/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.35169\n",
      "Epoch 396/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.35169\n",
      "Epoch 397/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.35169\n",
      "Epoch 398/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.35169\n",
      "Epoch 399/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.35169\n",
      "Epoch 400/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.35169\n",
      "Epoch 401/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.35169\n",
      "Epoch 402/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.35169\n",
      "Epoch 403/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 1.35169\n",
      "Epoch 404/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 1.35169\n",
      "Epoch 405/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 1.35169\n",
      "Epoch 406/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 1.35169\n",
      "Epoch 407/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 1.35169\n",
      "Epoch 408/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 1.35169\n",
      "Epoch 409/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 1.35169\n",
      "Epoch 410/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 1.35169\n",
      "Epoch 411/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 1.35169\n",
      "Epoch 412/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 1.35169\n",
      "Epoch 413/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 1.35169\n",
      "Epoch 414/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 1.35169\n",
      "Epoch 415/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 1.35169\n",
      "Epoch 416/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 1.35169\n",
      "Epoch 417/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 1.35169\n",
      "Epoch 418/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 1.35169\n",
      "Epoch 419/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 1.35169\n",
      "Epoch 420/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 1.35169\n",
      "Epoch 421/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 1.35169\n",
      "Epoch 422/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 1.35169\n",
      "Epoch 423/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 1.35169\n",
      "Epoch 424/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 1.35169\n",
      "Epoch 425/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 1.35169\n",
      "Epoch 426/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 1.35169\n",
      "Epoch 427/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 1.35169\n",
      "Epoch 428/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 1.35169\n",
      "Epoch 429/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 1.35169\n",
      "Epoch 430/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 1.35169\n",
      "Epoch 431/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 1.35169\n",
      "Epoch 432/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 1.35169\n",
      "Epoch 433/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 1.35169\n",
      "Epoch 434/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 1.35169\n",
      "Epoch 435/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 1.35169\n",
      "Epoch 436/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 1.35169\n",
      "Epoch 437/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 1.35169\n",
      "Epoch 438/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 1.35169\n",
      "Epoch 439/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 1.35169\n",
      "Epoch 440/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 1.35169\n",
      "Epoch 441/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 1.35169\n",
      "Epoch 442/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 1.35169\n",
      "Epoch 443/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 1.35169\n",
      "Epoch 444/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 1.35169\n",
      "Epoch 445/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 1.35169\n",
      "Epoch 446/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 1.35169\n",
      "Epoch 447/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 1.35169\n",
      "Epoch 448/450\n",
      "792/792 [==============================] - 70s 89ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 1.35169\n",
      "Epoch 449/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 1.35169\n",
      "Epoch 450/450\n",
      "792/792 [==============================] - 70s 88ms/step - loss: 15.4058 - fbeta: 0.0442 - val_loss: 15.6704 - val_fbeta: 0.0278\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 1.35169\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import callbacks\n",
    "\n",
    "PARAM_MAX_EPOCHS = 450 #500 # PARAM: number of model-fit runs\n",
    "PARAM_N_BATCH = 44 #62 #64 # PARAM: number of input samples for one feedfwd-backprop step\n",
    "PARAM_VAL_SPLIT = 6/39 #4/39\n",
    "\n",
    "checkpointer = callbacks.ModelCheckpoint (\n",
    "    filepath=os.path.join ('model','model.w.best.h5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit (X_train, y_train,\n",
    "                     epochs=PARAM_MAX_EPOCHS, batch_size=PARAM_N_BATCH, validation_split=PARAM_VAL_SPLIT,\n",
    "                     shuffle=True, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_fbeta', 'fbeta', 'val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "print (history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "# summarize history for accuracy\n",
    "axs[0].plot (history.history['fbeta'])\n",
    "if 'val_fbeta' in history.history:\n",
    "    axs[0].plot (history.history['val_fbeta'])\n",
    "axs[0].set (xlabel='epoch', ylabel='score (higher better)', title='F-{} score'.format (PARAM_BETA))\n",
    "axs[0].legend (['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "axs[1].plot (history.history['loss'])\n",
    "if 'val_loss' in history.history:\n",
    "    axs[1].plot (history.history['val_loss'])\n",
    "axs[1].set (xlabel='epoch', ylabel='loss (lower better)', title='loss (cat x-entropy)')\n",
    "axs[1].legend (['train', 'validation'], loc='upper left')\n",
    "\n",
    "fig.set_size_inches ((15., 6.), forward=True)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**discussion**\n",
    "\n",
    "[2018-09-02]\n",
    "\n",
    "epoch 205:\n",
    "- loss: 0.0280\n",
    "- val_loss: 0.4546\n",
    "- fbeta: 1.0000\n",
    "- val_fbeta: 0.9412\n",
    "\n",
    "_\n",
    "\n",
    "Above graphs show the F-beta score per epoch with beta = 1 on the left and the *loss per epoch*, calulated by the mean squared error (mse) on the right.\n",
    "\n",
    "*loss per epoch*:\n",
    "- gradient steps start with a loss of 0.052, end by 0.042 and show a smooth concave curve. The curve couldn't be better except a faster drop in the first 10 epochs.\n",
    "- the worse: mse after 1st epoch = 0.052 - the CNN learns very slow and in tiny steps (1st/2nd epoch: 0.052-0.049 = 0.003)\n",
    "\n",
    "*F-beta score per epoch*\n",
    "- evaluation metric immediately drops to zero after some epochs - the CNN doesn't learn anything yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reasons / todo\n",
    "\n",
    "*input data*\n",
    "\n",
    "(1) The used dataset only has 240 samples for training, validation and test. This is by far nothing for the CNN.\n",
    "\n",
    "Todo: retrieve more samples for the dataset\n",
    "\n",
    "(2) A quick look at random spectrograms show kind of chaotic information - as a human being it is hard to tell if there's any structure behind each key-mode pair. This may apply to the CNN too.\n",
    "\n",
    "Todo: find additional filter techniques / methods to clearly bring out structures for the CNN\n",
    "\n",
    "(3) Songs can change in key over their whole length.\n",
    "\n",
    "Todo: take appropriate sample of a song - ommit bridges, refrains, silent passages, noisy songs\n",
    "\n",
    "_\n",
    "\n",
    "*model training*\n",
    "\n",
    "The model was trained for 100 epochs, each in batches of 10 samples per feedfwd-backprop step. To make sure that the architecture is well suited, more epochs shall be run.\n",
    "\n",
    "Todo: increase epochs, change batch size\n",
    "\n",
    "_\n",
    "\n",
    "*model architecture*\n",
    "\n",
    "Todo: To better understand the insight of the CNN, visualize the filter of the convolutions. May there be enlightenment what kind of architecture works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare learning algorithm to benchmarks\n",
    "\n",
    "[i] below statements can be run without executing the whole notebook\n",
    "\n",
    "Therefor, go to and execute <a href='#load-learning-algorithm'>load learning algorithm</a>\n",
    "\n",
    "**TODO** **TODO** **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**benchmark data - fft**\n",
    "\n",
    "Juypter Notebook <a href='./00.hlp/fft/fft.ipynb#Benchmark-data'>benchmark data - fft</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'filenames', 'target', 'target_names'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD SPECTROGRAM FILENAMES\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_bench', 'src_spectro')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_bench_spectro_data = datasets.load_files (container_path=container_path,\n",
    "                                              description=description,\n",
    "                                              load_content=load_content,\n",
    "                                              random_state=PARAM_RND_STATE)\n",
    "src_bench_spectro_data.keys ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:01<00:00, 24.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "bench_spectro_tensors = paths_to_tensor (src_bench_spectro_data['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 35ms/step\n",
      "[('loss', 4.415348211924235), ('fbeta', 0.14260870218276978)]\n"
     ]
    }
   ],
   "source": [
    "X_test = bench_spectro_tensors\n",
    "y_test = np_utils.to_categorical (np.array (src_bench_spectro_data['target']), 24)\n",
    "\n",
    "model.load_weights (os.path.join ('model','model.w.best.h5'))\n",
    "\n",
    "score = model.evaluate (X_test, y_test, verbose=1)\n",
    "print ([(model.metrics_names[i], score[i]) for i in range (len (model.metrics_names))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cm</th>\n",
       "      <th>C</th>\n",
       "      <th>C#m</th>\n",
       "      <th>C#</th>\n",
       "      <th>Dm</th>\n",
       "      <th>D</th>\n",
       "      <th>D#m</th>\n",
       "      <th>D#</th>\n",
       "      <th>Em</th>\n",
       "      <th>E</th>\n",
       "      <th>Fm</th>\n",
       "      <th>F</th>\n",
       "      <th>F#m</th>\n",
       "      <th>F#</th>\n",
       "      <th>Gm</th>\n",
       "      <th>G</th>\n",
       "      <th>G#m</th>\n",
       "      <th>G#</th>\n",
       "      <th>Am</th>\n",
       "      <th>A</th>\n",
       "      <th>A#m</th>\n",
       "      <th>A#</th>\n",
       "      <th>Bm</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D#m</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Em</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F#m</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F#</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G#m</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Am</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A#</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cm  C  C#m  C#  Dm  D  D#m  D#  Em  E  Fm  F  F#m  F#  Gm  G  G#m  G#  \\\n",
       "Cm    0  0    1   0   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C     1  1    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C#m   0  0    1   0   1  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C#    0  0    0   0   0  0    0   0   0  0   0  0    0   0   1  0    0   1   \n",
       "Dm    1  0    0   0   1  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "D     0  0    0   0   0  1    0   0   0  0   0  0    0   0   1  0    0   0   \n",
       "D#m   1  0    0   0   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "D#    0  0    0   0   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "Em    0  0    0   0   2  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "E     0  0    0   0   0  1    0   0   1  0   0  0    0   0   0  0    0   0   \n",
       "Fm    0  0    0   0   0  1    0   0   0  0   0  1    0   0   0  0    0   0   \n",
       "F     0  0    0   1   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "F#m   1  0    0   0   1  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "F#    0  2    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "Gm    0  0    0   0   2  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "G     0  0    0   0   1  0    0   0   0  0   0  0    0   0   0  1    0   0   \n",
       "G#m   1  0    0   0   1  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "G#    0  0    0   0   1  0    0   0   1  0   0  0    0   0   0  0    0   0   \n",
       "Am    0  0    0   0   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "A     0  0    0   0   1  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "A#m   0  0    0   0   0  0    0   0   0  0   0  0    1   0   0  0    0   0   \n",
       "A#    0  2    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "Bm    0  0    0   0   0  0    0   0   0  0   1  0    0   0   0  0    0   0   \n",
       "B     0  0    1   0   0  0    0   0   0  0   0  0    0   0   0  1    0   0   \n",
       "\n",
       "     Am  A  A#m  A#  Bm  B  \n",
       "Cm    0  0    0   0   0  0  \n",
       "C     0  0    0   0   0  0  \n",
       "C#m   0  0    0   0   0  0  \n",
       "C#    0  0    0   0   0  0  \n",
       "Dm    0  0    0   0   0  0  \n",
       "D     0  0    0   0   0  0  \n",
       "D#m   0  0    0   0   0  0  \n",
       "D#    0  1    0   0   0  0  \n",
       "Em    0  0    0   0   0  0  \n",
       "E     0  0    0   0   0  0  \n",
       "Fm    0  0    0   0   0  0  \n",
       "F     0  0    0   0   0  0  \n",
       "F#m   0  0    0   0   0  0  \n",
       "F#    0  0    0   0   0  0  \n",
       "Gm    0  0    0   0   0  0  \n",
       "G     0  0    0   0   0  0  \n",
       "G#m   0  0    0   0   0  0  \n",
       "G#    0  0    0   0   0  0  \n",
       "Am    0  1    0   0   0  0  \n",
       "A     0  0    1   0   0  0  \n",
       "A#m   0  0    1   0   0  0  \n",
       "A#    0  0    0   0   0  0  \n",
       "Bm    0  1    0   0   0  0  \n",
       "B     0  0    0   0   0  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [y_test[i].argmax () for i in range (len (y_test))]\n",
    "model_pred = model.predict (X_test)\n",
    "y_pred = [model_pred[i].argmax () for i in range (len (model_pred))]\n",
    "\n",
    "cm = confusion_matrix (y_true, y_pred)\n",
    "cm_pd = pd.DataFrame (data=cm, index=src_bench_spectro_data['DESCR'], columns=src_bench_spectro_data['DESCR'])\n",
    "\n",
    "pd.set_option ('display.max_columns', 24)\n",
    "display (cm_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scikitplot as skplt\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix (y_true, y_pred, figsize=(14,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> saving model architecture... done\n",
      ">>> saving model weights... done\n",
      ">>> saving model history... done\n",
      ">>> saving model parameter... done\n"
     ]
    }
   ],
   "source": [
    "# serialization of model architecture\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "t = time.strftime (\"%Y%m%d-%H%M%S\", time.gmtime ())\n",
    "\n",
    "# save model architecture\n",
    "save_name = os.path.join ('model', t + '_model.arch' + '.yaml')\n",
    "print ('>>> saving model architecture...', end=' ', flush=True)\n",
    "yaml_string = model.to_yaml ()\n",
    "with open (save_name, 'w') as yaml_file:\n",
    "    yaml_file.write (yaml_string)\n",
    "print ('done')\n",
    "\n",
    "# save model weights\n",
    "save_name = os.path.join ('model', t + '_model.weights' + '.h5')\n",
    "print ('>>> saving model weights...', end=' ', flush=True)\n",
    "model.save_weights (save_name)\n",
    "print ('done')\n",
    "\n",
    "# save learning history\n",
    "save_name = os.path.join ('model', t + '_model.hist' + '.csv')\n",
    "print ('>>> saving model history...', end=' ', flush=True)\n",
    "hist_df = pd.DataFrame.from_dict (history.history)\n",
    "hist_df.to_csv (save_name)\n",
    "print ('done')\n",
    "\n",
    "# save parameter\n",
    "save_name = os.path.join ('model', t + '_model.param' + '.txt')\n",
    "print ('>>> saving model parameter...', end=' ', flush=True)\n",
    "str_to_write = str ({'seed':PARAM_RND_STATE, 'use_img_std':PARAM_USE_IMG_STD,\n",
    "                'opt':str (type (opt)), 'lr':PARAM_LR, 'mom':PARAM_MOM, 'loss':str (type (loss)),\n",
    "                'epochs':PARAM_MAX_EPOCHS, 'batch_size':PARAM_N_BATCH, 'val_split':PARAM_VAL_SPLIT})\n",
    "with open (save_name, 'w') as str_file:\n",
    "    str_file.write (str_to_write)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> loading and compiling model... done\n",
      ">>> loading best weights into model... done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 32)      160       \n",
      "_________________________________________________________________\n",
      "maxp_1 (MaxPooling2D)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 80, 80, 64)        8256      \n",
      "_________________________________________________________________\n",
      "maxp_2 (MaxPooling2D)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 40, 128)       32896     \n",
      "_________________________________________________________________\n",
      "maxp_3 (MaxPooling2D)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "avg_flatten (GlobalAveragePo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 24)                3096      \n",
      "=================================================================\n",
      "Total params: 44,408\n",
      "Trainable params: 44,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model architecture\n",
    "import os\n",
    "from keras import models\n",
    "\n",
    "load_name = os.path.join ('model', 'model.arch_180902.yaml')\n",
    "print ('>>> loading and compiling model...', end=' ', flush=True)\n",
    "with open (load_name, 'r') as yaml_file:\n",
    "    yaml_string = yaml_file.read ()\n",
    "model = models.model_from_yaml (yaml_string)\n",
    "#model.compile (optimizer=opt_sgd, loss=loss, metrics=[fbeta])\n",
    "print ('done')\n",
    "\n",
    "# load best weights\n",
    "print ('>>> loading best weights into model...', end=' ', flush=True)\n",
    "model.load_weights (os.path.join ('model','model.w.best_180902.h5'))\n",
    "print ('done')\n",
    "\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_spectro/5-1/TRCWNLD128F93011AD_0-oct.png\n",
      "y_true 5-1\n",
      "y_pred 5-1\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "test_file = src_spectro_data['filenames'][idx]\n",
    "\n",
    "test_spectro = path_to_tensor (test_file)\n",
    "test_pred = model.predict (test_spectro)\n",
    "\n",
    "print (test_file)\n",
    "print ('y_true', src_spectro_data['target_names'][src_spectro_data['target'][idx]])\n",
    "print ('y_pred', src_spectro_data['target_names'][test_pred.argmax ()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**drawbacks** (known, unresolvable issues)\n",
    "\n",
    "(WRONG) *music keys vs CNN key classes*\n",
    "\n",
    "See <a href='https://www.researchgate.net/publication/228963946_Audio_onset_detection_using_machine_learning_techniques_the_effect_and_applicability_of_key_and_tempo_information'>Chuan, Ching-Hua & Chew, Elaine. (2018). Audio onset detection using machine learning techniques: the effect and applicability of key and tempo information.</a>, p. 18\n",
    "\n",
    "The spectrograms show a pitch range given by the <a href='https://en.wikipedia.org/wiki/Scientific_pitch_notation#Table_of_note_frequencies'>Scientific Pitch Notation</a>. By that the range of notes goes from $C_{-1}$ = $0_{MIDI}$ up to $G_9$ = $127_{MIDI}$.\n",
    "\n",
    "Each note can be the tonic of a music key - for example the key 'C major' exists 11 times (ocatve -1 to 9). Thus the information of 128 keys is now squeezed into 24 key classes.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
