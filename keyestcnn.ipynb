{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio key estimation of digital music with CNNs\n",
    "Udacity Machine Learning Nanodegree - Capstone project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "The project is structured as stated in section 'Project Design' of the Capstone project proposal.\n",
    "\n",
    "_\n",
    "<pre>\n",
    "<a href='#Data-Preprocessing'>Data Preprocessing</a>\n",
    "  <a href='#Million-Song-Dataset'>Million Song Dataset</a> - selection of appropriate songs, separate jupyter notebook\n",
    "  <a href='#Signal-Processing-and-Feature-Extraction'>Signal Processing and Feature Extraction</a> - separate jupyter notebook\n",
    "\n",
    "<a href='#Model-Preparation'>Model Preparation</a>\n",
    "  <a href='#Load-and-preprocess-data'>Load and preprocess data</a> - read spectrogram images, conversion to tensors\n",
    "  <a href='#Split-data-into-train-and-test-set'>Splitting data into training/testing sets</a>\n",
    "  <a href='#Model-architecture'>CNN model architecture</a>\n",
    "  <a href='#Model-parameter'>CNN model parameter</a>\n",
    "\n",
    "<a href='#Model-Training-and-Evaluation'>Model Training and Evaluation</a>\n",
    "  <a href='#Model-training'>Model training</a>\n",
    "  <a href='#Model-evaluation-and-comparison'>Model evaluation and comparison</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current version of the project is working, but\n",
    "\n",
    "the project is still ongoing...\n",
    "\n",
    "discussion and remarks of what to do can be found in section\n",
    "\n",
    "<a href='#reasons-/-todo'>reasons / todo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Million Song Dataset\n",
    "- utilized to select appropriate song samples\n",
    "- holds information about key and mode per song (targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juypter Notebook <a href='./00.hlp/msd/msd.ipynb'>msd</a>\n",
    "\n",
    "outputs: csv file *songs_conf=75_tracks_filt.csv*, which holds all songs with key confidence and mode confidence > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>track_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.852</td>\n",
       "      <td>114.493</td>\n",
       "      <td>TRMMMGL128F92FD6AB</td>\n",
       "      <td>SOHSSPG12A8C144BE0</td>\n",
       "      <td>Clifford T. Ward</td>\n",
       "      <td>Mad About You</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key  key_confidence  mode  mode_confidence    tempo            track_id  \\\n",
       "0    7           0.896     1            0.852  114.493  TRMMMGL128F92FD6AB   \n",
       "\n",
       "              song_id       artist_name     song_title  \n",
       "0  SOHSSPG12A8C144BE0  Clifford T. Ward  Mad About You  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of records: 47913\n"
     ]
    }
   ],
   "source": [
    "# LIST SELECTED SONGS\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "selsongsfile = os.path.join ('00.hlp', 'msd', 'songs_conf=75_tracks_filt.csv')\n",
    "selsongs = pd.read_csv (selsongsfile, header=0, index_col=0)\n",
    "display (selsongs.head (1))\n",
    "print ('[i] number of records:', len (selsongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AUDIO DATASET\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_audio')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_audio_data = datasets.load_files (container_path=container_path,\n",
    "                                      description=description,\n",
    "                                      load_content=load_content,\n",
    "                                      random_state=PARAM_RND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>track_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>178.571</td>\n",
       "      <td>TRMCBGQ128F425C0A8</td>\n",
       "      <td>SOHGQTL12A8C139348</td>\n",
       "      <td>Jo-El Sonnier</td>\n",
       "      <td>The Back Door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29273</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889</td>\n",
       "      <td>132.947</td>\n",
       "      <td>TRUXMRH128F92F7A43</td>\n",
       "      <td>SOGTREL12AB0181DC8</td>\n",
       "      <td>Blues Company</td>\n",
       "      <td>Dark Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21568</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.847</td>\n",
       "      <td>153.823</td>\n",
       "      <td>TRAVHZZ128F424BEDA</td>\n",
       "      <td>SONSGIT12A8C138856</td>\n",
       "      <td>Heldon</td>\n",
       "      <td>In Wake of King Fripp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46577</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965</td>\n",
       "      <td>95.543</td>\n",
       "      <td>TRYFBHO128F422B190</td>\n",
       "      <td>SONEEWO12A58A7F42A</td>\n",
       "      <td>Tom Rush</td>\n",
       "      <td>Rockport Sunday [Remastered Album Version]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>101.146</td>\n",
       "      <td>TRRJCHZ128F4270606</td>\n",
       "      <td>SOANZZR12A8C134DE5</td>\n",
       "      <td>Vivian Khor</td>\n",
       "      <td>Ancient Wanderer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  key_confidence  mode  mode_confidence    tempo  \\\n",
       "311      7             1.0     1            1.000  178.571   \n",
       "29273    0             1.0     0            0.889  132.947   \n",
       "21568    3             1.0     0            0.847  153.823   \n",
       "46577   11             1.0     1            0.965   95.543   \n",
       "10322    1             1.0     1            1.000  101.146   \n",
       "\n",
       "                 track_id             song_id    artist_name  \\\n",
       "311    TRMCBGQ128F425C0A8  SOHGQTL12A8C139348  Jo-El Sonnier   \n",
       "29273  TRUXMRH128F92F7A43  SOGTREL12AB0181DC8  Blues Company   \n",
       "21568  TRAVHZZ128F424BEDA  SONSGIT12A8C138856         Heldon   \n",
       "46577  TRYFBHO128F422B190  SONEEWO12A58A7F42A       Tom Rush   \n",
       "10322  TRRJCHZ128F4270606  SOANZZR12A8C134DE5    Vivian Khor   \n",
       "\n",
       "                                       song_title  \n",
       "311                                 The Back Door  \n",
       "29273                                    Dark Day  \n",
       "21568                       In Wake of King Fripp  \n",
       "46577  Rockport Sunday [Remastered Album Version]  \n",
       "10322                            Ancient Wanderer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of records: 312\n",
      "[i] min of: key_confidence = 0.825 , mode_confidence = 0.777\n",
      "[i] tempo: min = 0.0 , max = 248.32299999999998\n"
     ]
    }
   ],
   "source": [
    "# FYI: LIST SOME OF THE USED SONGS\n",
    "filenames = list (os.path.basename (filepath) for filepath in src_audio_data['filenames'])\n",
    "usedsongs_track_id = list (os.path.splitext (fn)[0] for fn in filenames)\n",
    "usedsongs = selsongs.query ('track_id in @usedsongs_track_id')\n",
    "\n",
    "display (usedsongs.sample(5))\n",
    "print ('[i] number of records:', len (usedsongs))\n",
    "print ('[i] min of: key_confidence =', usedsongs['key_confidence'].min (), ',', \\\n",
    "       'mode_confidence =', usedsongs['mode_confidence'].min ())\n",
    "print ('[i] tempo: min =', usedsongs['tempo'].min (), ',', \\\n",
    "       'max =', usedsongs['tempo'].max ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- save list of used songs\n",
    "usedsongs.to_csv ('usedsongs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- save list of unused songs\n",
    "unusedsongs = selsongs.drop ((usedsongs.index.values))\n",
    "unusedsongs.to_csv ('unusedsongs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing and Feature Extraction\n",
    "- create spectrograms of audio files with discrete Fourier transform (DFT)\n",
    "- save spectrograms as images for further use in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juypter Notebook <a href='./00.hlp/fft/fft.ipynb'>fft</a>\n",
    "\n",
    "ouptuts: spectrograms (png images) of audio files with same folder structure as *src_audio* in new container path named *src_spectro*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of a spectrogram image**\n",
    "\n",
    "<img src ='./src_spectro/7-0/TREDRTV12903D03829.png' align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Since there aren't enough samples to proper train the classifier, image augmentation is used.\n",
    "\n",
    "Below jupyter notebook does the work.\n",
    "\n",
    "Juypter Notebook <a href='./00.hlp/trnsp/trnsp.ipynb'>transposing songs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filenames', 'target', 'target_names', 'DESCR'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD SPECTROGRAM FILENAMES\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_spectro')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_spectro_data = datasets.load_files (container_path=container_path,\n",
    "                                        description=description,\n",
    "                                        load_content=load_content,\n",
    "                                        random_state=PARAM_RND_STATE)\n",
    "src_spectro_data.keys ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] example of loaded spectrogram file data:\n",
      "    spectrogram image name: src_spectro/2-0/TREFDWU128F9303DF8.png\n",
      "    spectrogram image key-mode pair: 2-0 = Em = target class 8\n"
     ]
    }
   ],
   "source": [
    "print ('[i] example of loaded spectrogram file data:')\n",
    "print ('    spectrogram image name:', src_spectro_data['filenames'][0])\n",
    "print ('    spectrogram image key-mode pair:',\\\n",
    "       src_spectro_data['target_names'][src_spectro_data['target'][0]],\\\n",
    "       '=', src_spectro_data['DESCR'][src_spectro_data['target'][0]],\\\n",
    "       '= target class', src_spectro_data['target'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in images, convert to tensors**\n",
    "\n",
    "Keras Conv2D layers expect a **4D tensor with shape (batch, rows, cols, channels)** (if param data_format='channels_last') (src: <a href='https://keras.io/layers/convolutional/#conv2d'>Keras Conv2D</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] image size: (150, 128)\n",
      "[i] pixel format: RGB\n"
     ]
    }
   ],
   "source": [
    "# open a random image and take a look at the attributes\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open (src_spectro_data['filenames'][0])\n",
    "print ('[i] image size:', im.size)\n",
    "print ('[i] pixel format:', im.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing target size of image**\n",
    "\n",
    "CNNs work best if input size is divisible by 2 many times - image size needs to be changed. (<a href='http://cs231n.github.io/convolutional-networks/#layersizepat'>cs231n - Layer Sizing Patterns</a>)\n",
    "\n",
    "Current image size is 150 x 128: possible options\n",
    "- (-) cut down the image to 128 x 128: information loss in song length\n",
    "- (-) resize to 150 x 150: not divisible by 2 many times (exactly 1 time)\n",
    "- (+) resize to 160 x 160: divisible by 2 many times (exactly 5 times, this is enough)\n",
    "\n",
    "Resizing is done by appending zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PARAM_TARGET_SIZE = 160\n",
    "def resize_image (img_arr):\n",
    "    #print ('>>> resizing image to [{}, {}]...'.format (PARAM_IMG_SIZE, PARAM_IMG_SIZE), end=' ', flush=True)\n",
    "    \n",
    "    m = img_arr.shape[0]\n",
    "    n = img_arr.shape[1]\n",
    "    \n",
    "    # how many additional cols to add?\n",
    "    cols_to_add = PARAM_TARGET_SIZE - n\n",
    "    \n",
    "    img_resized = np.empty ((1, PARAM_TARGET_SIZE))\n",
    "    for i in range (m):\n",
    "        new_line = np.append (img_arr[i], np.zeros (cols_to_add))\n",
    "        img_resized = np.vstack ((img_resized, new_line))\n",
    "    \n",
    "    img_resized = img_resized[1:]\n",
    "    # now img_resized = (128, 160)\n",
    "    \n",
    "    # how many additional rows to add?\n",
    "    rows_to_add = PARAM_TARGET_SIZE - m\n",
    "    img_resized = np.vstack ((img_resized, np.zeros ((rows_to_add, PARAM_TARGET_SIZE))))\n",
    "    # now img_resize = (160, 160)\n",
    "    \n",
    "    #print ('done')\n",
    "    \n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling by feature standardization**\n",
    "\n",
    "$x^{'}= \\frac{x-\\bar{x}}{\\sigma}$\n",
    "\n",
    "tensorflow function used to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below functions read the images and convert those to tensors - original code taken from Udacity MLND dog-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor (img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img (img_path, color_mode='grayscale')\n",
    "    \n",
    "    # convert PIL.Image.Image type to 3D tensor\n",
    "    x = image.img_to_array (img)\n",
    "    x = resize_image (x)\n",
    "    x = x[:,:,np.newaxis]\n",
    "    \n",
    "    # feature standardization to zero mean and stdev of one\n",
    "    # [2018-09-04] turned out that standardization prevents good learning progress\n",
    "    #x = K.eval (tf.image.per_image_standardization (x))\n",
    "    \n",
    "    # convert 3D tensor to 4D tensor\n",
    "    return np.expand_dims (x, axis=0)\n",
    "\n",
    "def paths_to_tensor (img_paths):\n",
    "    list_of_tensors = [path_to_tensor (img_path) for img_path in tqdm (img_paths)]\n",
    "    return np.vstack (list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 936/936 [00:02<00:00, 360.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "spectro_tensors = paths_to_tensor (src_spectro_data['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] shape of spectrogram tensors: (936, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('[i] shape of spectrogram tensors:', spectro_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] number of output classes: 24\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "targets = np_utils.to_categorical (np.array (src_spectro_data['target']), 24)\n",
    "print ('[i] number of output classes:', targets.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test set\n",
    "[2018-09-03] obsolete since test data is in separate directory now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Training dataset consists of 936 samples\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = \\\n",
    "#    train_test_split (spectro_tensors, targets, test_size=(8/39), shuffle=True, random_state=PARAM_RND_STATE)\n",
    "\n",
    "#print ('[i] Training dataset consists of {} samples'.format (X_train.shape[0]))\n",
    "#print ('[i] Testing dataset consists of {} samples'.format (X_test.shape[0]))\n",
    "\n",
    "X_train = spectro_tensors\n",
    "y_train = targets\n",
    "print ('[i] Training dataset consists of {} samples'.format (X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "(<a href='http://cs231n.github.io/convolutional-networks/#layersizepat'>cs231n - Layer Sizing Patterns</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 16)      80        \n",
      "_________________________________________________________________\n",
      "maxp_1 (MaxPooling2D)        (None, 80, 80, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 80, 80, 64)        4160      \n",
      "_________________________________________________________________\n",
      "maxp_2 (MaxPooling2D)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 40, 128)       32896     \n",
      "_________________________________________________________________\n",
      "maxp_3 (MaxPooling2D)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 256)       131328    \n",
      "_________________________________________________________________\n",
      "maxp_4 (MaxPooling2D)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "avg_flatten (GlobalAveragePo (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 24)                6168      \n",
      "=================================================================\n",
      "Total params: 174,632\n",
      "Trainable params: 174,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# clear everything known of past instances (\"useful to avoid clutter from old models / layers\")\n",
    "K.clear_session ()\n",
    "\n",
    "# input layer\n",
    "inputs = layers.Input (shape=spectro_tensors.shape[1:], name='input')\n",
    "\n",
    "# hidden layers\n",
    "net = layers.Conv2D (filters=16, kernel_size=(2,2), strides=(1,1),\n",
    "                     padding='same', # don't lose information due to conv window runs out of image / strides = 1 = OK\n",
    "                     activation='relu',\n",
    "                     name='conv2d_1') (inputs)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_1') (net)\n",
    "\n",
    "net = layers.Conv2D (filters=64, kernel_size=(2,2), strides=(1,1),\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              name='conv2d_2') (net)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_2') (net)\n",
    "\n",
    "net = layers.Conv2D (filters=128, kernel_size=(2,2), strides=(1,1),\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              name='conv2d_3') (net)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_3') (net)\n",
    "\n",
    "net = layers.Conv2D (filters=256, kernel_size=(2,2), strides=(1,1),\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              name='conv2d_4') (net)\n",
    "net = layers.MaxPooling2D (pool_size=(2,2), strides=None, name='maxp_4') (net)\n",
    "\n",
    "\n",
    "# 'flatten layer'\n",
    "net = layers.GlobalAveragePooling2D (name='avg_flatten') (net)\n",
    "\n",
    "net = layers.Dropout (0.25, seed=PARAM_RND_STATE) (net) # [2018-09-04] dropout to prevent overfitting\n",
    "\n",
    "# output layer\n",
    "outputs = layers.Dense (units=targets.shape[1], activation='softmax', name='output') (net)\n",
    "\n",
    "\n",
    "model = models.Model (inputs=inputs, outputs=outputs)\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter\n",
    "(metric, loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: Arseny Kravchenko http://arseny.info/2017/f-beta-score-for-keras.html\n",
    "from keras import backend as K\n",
    "\n",
    "PARAM_BETA = 1\n",
    "def fbeta (y_true, y_pred):\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip (y_pred, 0, 1)\n",
    "\n",
    "    tp = K.sum (K.round (y_true * y_pred)) + K.epsilon ()\n",
    "    fp = K.sum (K.round (K.clip (y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum (K.round (K.clip (y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = PARAM_BETA ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, losses\n",
    "\n",
    "# old params: lr=0.001, mom=0.8\n",
    "PARAM_LR = 0.0005\n",
    "PARAM_MOM = 0.95\n",
    "opt_sgd = optimizers.SGD (lr=PARAM_LR, momentum=PARAM_MOM)\n",
    "\n",
    "loss = losses.categorical_crossentropy\n",
    "\n",
    "model.compile (optimizer=opt_sgd, loss=loss, metrics=[fbeta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 168 samples\n",
      "Epoch 1/500\n",
      "768/768 [==============================] - 28s 37ms/step - loss: 5.0799 - fbeta: 0.0083 - val_loss: 3.6021 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.60206, saving model to model/model.w.best.h5\n",
      "Epoch 2/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.5481 - fbeta: 3.0898e-09 - val_loss: 3.2099 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.60206 to 3.20985, saving model to model/model.w.best.h5\n",
      "Epoch 3/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.2124 - fbeta: 3.1250e-09 - val_loss: 3.1841 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.20985 to 3.18406, saving model to model/model.w.best.h5\n",
      "Epoch 4/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1882 - fbeta: 3.1250e-09 - val_loss: 3.1852 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.18406\n",
      "Epoch 5/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1827 - fbeta: 3.1250e-09 - val_loss: 3.1853 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.18406\n",
      "Epoch 6/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1756 - fbeta: 3.1250e-09 - val_loss: 3.1847 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.18406\n",
      "Epoch 7/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1699 - fbeta: 3.1250e-09 - val_loss: 3.1833 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.18406 to 3.18333, saving model to model/model.w.best.h5\n",
      "Epoch 8/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1719 - fbeta: 3.1250e-09 - val_loss: 3.1828 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.18333 to 3.18278, saving model to model/model.w.best.h5\n",
      "Epoch 9/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1691 - fbeta: 3.1250e-09 - val_loss: 3.1821 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.18278 to 3.18206, saving model to model/model.w.best.h5\n",
      "Epoch 10/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1652 - fbeta: 3.1250e-09 - val_loss: 3.1818 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.18206 to 3.18177, saving model to model/model.w.best.h5\n",
      "Epoch 11/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1651 - fbeta: 3.1250e-09 - val_loss: 3.1816 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.18177 to 3.18161, saving model to model/model.w.best.h5\n",
      "Epoch 12/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1636 - fbeta: 3.1250e-09 - val_loss: 3.1812 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.18161 to 3.18123, saving model to model/model.w.best.h5\n",
      "Epoch 13/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1635 - fbeta: 3.1250e-09 - val_loss: 3.1812 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.18123 to 3.18122, saving model to model/model.w.best.h5\n",
      "Epoch 14/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1630 - fbeta: 3.1250e-09 - val_loss: 3.1814 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.18122\n",
      "Epoch 15/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1650 - fbeta: 3.1250e-09 - val_loss: 3.1815 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.18122\n",
      "Epoch 16/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1620 - fbeta: 3.1250e-09 - val_loss: 3.1813 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.18122\n",
      "Epoch 17/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1644 - fbeta: 3.1250e-09 - val_loss: 3.1809 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.18122 to 3.18091, saving model to model/model.w.best.h5\n",
      "Epoch 18/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1555 - fbeta: 3.1250e-09 - val_loss: 3.1802 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.18091 to 3.18020, saving model to model/model.w.best.h5\n",
      "Epoch 19/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1576 - fbeta: 3.1250e-09 - val_loss: 3.1783 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.18020 to 3.17825, saving model to model/model.w.best.h5\n",
      "Epoch 20/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1567 - fbeta: 3.1250e-09 - val_loss: 3.1780 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.17825 to 3.17797, saving model to model/model.w.best.h5\n",
      "Epoch 21/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1566 - fbeta: 3.1250e-09 - val_loss: 3.1780 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3.17797\n",
      "Epoch 22/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1533 - fbeta: 3.1250e-09 - val_loss: 3.1775 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.17797 to 3.17750, saving model to model/model.w.best.h5\n",
      "Epoch 23/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1503 - fbeta: 3.1250e-09 - val_loss: 3.1772 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.17750 to 3.17722, saving model to model/model.w.best.h5\n",
      "Epoch 24/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1540 - fbeta: 3.1250e-09 - val_loss: 3.1773 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.17722\n",
      "Epoch 25/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1467 - fbeta: 3.1250e-09 - val_loss: 3.1765 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.17722 to 3.17654, saving model to model/model.w.best.h5\n",
      "Epoch 26/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1439 - fbeta: 3.1250e-09 - val_loss: 3.1755 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.17654 to 3.17550, saving model to model/model.w.best.h5\n",
      "Epoch 27/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1426 - fbeta: 3.1250e-09 - val_loss: 3.1742 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.17550 to 3.17421, saving model to model/model.w.best.h5\n",
      "Epoch 28/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1402 - fbeta: 3.1250e-09 - val_loss: 3.1733 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.17421 to 3.17334, saving model to model/model.w.best.h5\n",
      "Epoch 29/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1419 - fbeta: 3.1250e-09 - val_loss: 3.1714 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00029: val_loss improved from 3.17334 to 3.17143, saving model to model/model.w.best.h5\n",
      "Epoch 30/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1365 - fbeta: 3.1250e-09 - val_loss: 3.1686 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.17143 to 3.16856, saving model to model/model.w.best.h5\n",
      "Epoch 31/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1407 - fbeta: 3.1250e-09 - val_loss: 3.1661 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00031: val_loss improved from 3.16856 to 3.16614, saving model to model/model.w.best.h5\n",
      "Epoch 32/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1308 - fbeta: 3.1250e-09 - val_loss: 3.1669 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.16614\n",
      "Epoch 33/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1221 - fbeta: 3.1250e-09 - val_loss: 3.1659 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00033: val_loss improved from 3.16614 to 3.16588, saving model to model/model.w.best.h5\n",
      "Epoch 34/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1276 - fbeta: 3.1250e-09 - val_loss: 3.1638 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.16588 to 3.16380, saving model to model/model.w.best.h5\n",
      "Epoch 35/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1300 - fbeta: 3.1250e-09 - val_loss: 3.1607 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00035: val_loss improved from 3.16380 to 3.16066, saving model to model/model.w.best.h5\n",
      "Epoch 36/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 26s 34ms/step - loss: 3.1240 - fbeta: 3.1250e-09 - val_loss: 3.1589 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.16066 to 3.15890, saving model to model/model.w.best.h5\n",
      "Epoch 37/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1239 - fbeta: 3.1250e-09 - val_loss: 3.1554 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.15890 to 3.15543, saving model to model/model.w.best.h5\n",
      "Epoch 38/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1168 - fbeta: 3.1250e-09 - val_loss: 3.1524 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.15543 to 3.15241, saving model to model/model.w.best.h5\n",
      "Epoch 39/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0996 - fbeta: 3.1250e-09 - val_loss: 3.1482 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00039: val_loss improved from 3.15241 to 3.14823, saving model to model/model.w.best.h5\n",
      "Epoch 40/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1103 - fbeta: 3.1250e-09 - val_loss: 3.1439 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00040: val_loss improved from 3.14823 to 3.14395, saving model to model/model.w.best.h5\n",
      "Epoch 41/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.1039 - fbeta: 3.1250e-09 - val_loss: 3.1385 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00041: val_loss improved from 3.14395 to 3.13852, saving model to model/model.w.best.h5\n",
      "Epoch 42/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0891 - fbeta: 3.1250e-09 - val_loss: 3.1327 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.13852 to 3.13268, saving model to model/model.w.best.h5\n",
      "Epoch 43/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0854 - fbeta: 3.1250e-09 - val_loss: 3.1313 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00043: val_loss improved from 3.13268 to 3.13131, saving model to model/model.w.best.h5\n",
      "Epoch 44/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0794 - fbeta: 3.1250e-09 - val_loss: 3.1284 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00044: val_loss improved from 3.13131 to 3.12845, saving model to model/model.w.best.h5\n",
      "Epoch 45/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0641 - fbeta: 3.1250e-09 - val_loss: 3.1270 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.12845 to 3.12702, saving model to model/model.w.best.h5\n",
      "Epoch 46/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0566 - fbeta: 3.1250e-09 - val_loss: 3.1147 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00046: val_loss improved from 3.12702 to 3.11471, saving model to model/model.w.best.h5\n",
      "Epoch 47/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0535 - fbeta: 3.1250e-09 - val_loss: 3.1115 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.11471 to 3.11149, saving model to model/model.w.best.h5\n",
      "Epoch 48/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0421 - fbeta: 3.1250e-09 - val_loss: 3.1051 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00048: val_loss improved from 3.11149 to 3.10512, saving model to model/model.w.best.h5\n",
      "Epoch 49/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0278 - fbeta: 3.1250e-09 - val_loss: 3.0913 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00049: val_loss improved from 3.10512 to 3.09134, saving model to model/model.w.best.h5\n",
      "Epoch 50/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0137 - fbeta: 3.1250e-09 - val_loss: 3.0816 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00050: val_loss improved from 3.09134 to 3.08163, saving model to model/model.w.best.h5\n",
      "Epoch 51/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 3.0035 - fbeta: 3.1250e-09 - val_loss: 3.0776 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00051: val_loss improved from 3.08163 to 3.07757, saving model to model/model.w.best.h5\n",
      "Epoch 52/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9941 - fbeta: 3.1250e-09 - val_loss: 3.0626 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00052: val_loss improved from 3.07757 to 3.06257, saving model to model/model.w.best.h5\n",
      "Epoch 53/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9740 - fbeta: 3.1250e-09 - val_loss: 3.0536 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00053: val_loss improved from 3.06257 to 3.05356, saving model to model/model.w.best.h5\n",
      "Epoch 54/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9555 - fbeta: 3.1250e-09 - val_loss: 3.0377 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00054: val_loss improved from 3.05356 to 3.03767, saving model to model/model.w.best.h5\n",
      "Epoch 55/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9489 - fbeta: 3.1250e-09 - val_loss: 3.0251 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00055: val_loss improved from 3.03767 to 3.02505, saving model to model/model.w.best.h5\n",
      "Epoch 56/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9167 - fbeta: 3.1250e-09 - val_loss: 3.0174 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00056: val_loss improved from 3.02505 to 3.01740, saving model to model/model.w.best.h5\n",
      "Epoch 57/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.9012 - fbeta: 3.1250e-09 - val_loss: 2.9990 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00057: val_loss improved from 3.01740 to 2.99901, saving model to model/model.w.best.h5\n",
      "Epoch 58/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.8701 - fbeta: 3.1250e-09 - val_loss: 2.9559 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00058: val_loss improved from 2.99901 to 2.95591, saving model to model/model.w.best.h5\n",
      "Epoch 59/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.8692 - fbeta: 3.1250e-09 - val_loss: 2.9397 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00059: val_loss improved from 2.95591 to 2.93965, saving model to model/model.w.best.h5\n",
      "Epoch 60/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.8374 - fbeta: 3.1250e-09 - val_loss: 2.9190 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00060: val_loss improved from 2.93965 to 2.91896, saving model to model/model.w.best.h5\n",
      "Epoch 61/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.7933 - fbeta: 3.1250e-09 - val_loss: 2.9094 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00061: val_loss improved from 2.91896 to 2.90939, saving model to model/model.w.best.h5\n",
      "Epoch 62/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.7521 - fbeta: 3.1250e-09 - val_loss: 2.8840 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00062: val_loss improved from 2.90939 to 2.88400, saving model to model/model.w.best.h5\n",
      "Epoch 63/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.7407 - fbeta: 3.1250e-09 - val_loss: 2.8445 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00063: val_loss improved from 2.88400 to 2.84454, saving model to model/model.w.best.h5\n",
      "Epoch 64/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.7052 - fbeta: 3.1250e-09 - val_loss: 2.8141 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00064: val_loss improved from 2.84454 to 2.81408, saving model to model/model.w.best.h5\n",
      "Epoch 65/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.6717 - fbeta: 0.0026 - val_loss: 2.8298 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.81408\n",
      "Epoch 66/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.6352 - fbeta: 0.0051 - val_loss: 2.7523 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00066: val_loss improved from 2.81408 to 2.75233, saving model to model/model.w.best.h5\n",
      "Epoch 67/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.6069 - fbeta: 0.0051 - val_loss: 2.7717 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.75233\n",
      "Epoch 68/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.5627 - fbeta: 0.0127 - val_loss: 2.7200 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00068: val_loss improved from 2.75233 to 2.72001, saving model to model/model.w.best.h5\n",
      "Epoch 69/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.5583 - fbeta: 0.0202 - val_loss: 2.7005 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00069: val_loss improved from 2.72001 to 2.70046, saving model to model/model.w.best.h5\n",
      "Epoch 70/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 2.5449 - fbeta: 0.0176 - val_loss: 2.6732 - val_fbeta: 3.5531e-09\n",
      "\n",
      "Epoch 00070: val_loss improved from 2.70046 to 2.67323, saving model to model/model.w.best.h5\n",
      "Epoch 71/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.4762 - fbeta: 0.0279 - val_loss: 2.6636 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00071: val_loss improved from 2.67323 to 2.66360, saving model to model/model.w.best.h5\n",
      "Epoch 72/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 2.4649 - fbeta: 0.0302 - val_loss: 2.6527 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00072: val_loss improved from 2.66360 to 2.65273, saving model to model/model.w.best.h5\n",
      "Epoch 73/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.4587 - fbeta: 0.0253 - val_loss: 2.6346 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00073: val_loss improved from 2.65273 to 2.63456, saving model to model/model.w.best.h5\n",
      "Epoch 74/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.4186 - fbeta: 0.0397 - val_loss: 2.5921 - val_fbeta: 3.5531e-09\n",
      "\n",
      "Epoch 00074: val_loss improved from 2.63456 to 2.59209, saving model to model/model.w.best.h5\n",
      "Epoch 75/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.3941 - fbeta: 0.0572 - val_loss: 2.6040 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.59209\n",
      "Epoch 76/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.3720 - fbeta: 0.0348 - val_loss: 2.5814 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00076: val_loss improved from 2.59209 to 2.58137, saving model to model/model.w.best.h5\n",
      "Epoch 77/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.3112 - fbeta: 0.0373 - val_loss: 2.5492 - val_fbeta: 3.5531e-09\n",
      "\n",
      "Epoch 00077: val_loss improved from 2.58137 to 2.54916, saving model to model/model.w.best.h5\n",
      "Epoch 78/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.2485 - fbeta: 0.0585 - val_loss: 2.5216 - val_fbeta: 3.5714e-09\n",
      "\n",
      "Epoch 00078: val_loss improved from 2.54916 to 2.52161, saving model to model/model.w.best.h5\n",
      "Epoch 79/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.2615 - fbeta: 0.0723 - val_loss: 2.5008 - val_fbeta: 3.5531e-09\n",
      "\n",
      "Epoch 00079: val_loss improved from 2.52161 to 2.50081, saving model to model/model.w.best.h5\n",
      "Epoch 80/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.2352 - fbeta: 0.0750 - val_loss: 2.5076 - val_fbeta: 3.5354e-09\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.50081\n",
      "Epoch 81/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.2710 - fbeta: 0.0936 - val_loss: 2.4715 - val_fbeta: 3.5531e-09\n",
      "\n",
      "Epoch 00081: val_loss improved from 2.50081 to 2.47152, saving model to model/model.w.best.h5\n",
      "Epoch 82/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.1925 - fbeta: 0.0681 - val_loss: 2.5472 - val_fbeta: 0.0221\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.47152\n",
      "Epoch 83/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.1061 - fbeta: 0.1334 - val_loss: 2.4604 - val_fbeta: 3.5354e-09\n",
      "\n",
      "Epoch 00083: val_loss improved from 2.47152 to 2.46043, saving model to model/model.w.best.h5\n",
      "Epoch 84/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.1180 - fbeta: 0.1457 - val_loss: 2.5281 - val_fbeta: 0.0233\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.46043\n",
      "Epoch 85/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.0999 - fbeta: 0.1546 - val_loss: 2.3955 - val_fbeta: 0.0218\n",
      "\n",
      "Epoch 00085: val_loss improved from 2.46043 to 2.39548, saving model to model/model.w.best.h5\n",
      "Epoch 86/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.0808 - fbeta: 0.1192 - val_loss: 2.4217 - val_fbeta: 0.0221\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.39548\n",
      "Epoch 87/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 2.0229 - fbeta: 0.1830 - val_loss: 2.3804 - val_fbeta: 0.0230\n",
      "\n",
      "Epoch 00087: val_loss improved from 2.39548 to 2.38041, saving model to model/model.w.best.h5\n",
      "Epoch 88/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.9843 - fbeta: 0.1465 - val_loss: 2.3789 - val_fbeta: 0.0221\n",
      "\n",
      "Epoch 00088: val_loss improved from 2.38041 to 2.37888, saving model to model/model.w.best.h5\n",
      "Epoch 89/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.9327 - fbeta: 0.1919 - val_loss: 2.3532 - val_fbeta: 0.0112\n",
      "\n",
      "Epoch 00089: val_loss improved from 2.37888 to 2.35324, saving model to model/model.w.best.h5\n",
      "Epoch 90/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.9071 - fbeta: 0.2022 - val_loss: 2.4110 - val_fbeta: 0.0322\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.35324\n",
      "Epoch 91/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.8999 - fbeta: 0.2136 - val_loss: 2.3989 - val_fbeta: 0.0334\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.35324\n",
      "Epoch 92/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.8974 - fbeta: 0.2228 - val_loss: 2.2567 - val_fbeta: 0.0565\n",
      "\n",
      "Epoch 00092: val_loss improved from 2.35324 to 2.25670, saving model to model/model.w.best.h5\n",
      "Epoch 93/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.8520 - fbeta: 0.2274 - val_loss: 2.3660 - val_fbeta: 0.0548\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.25670\n",
      "Epoch 94/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.8133 - fbeta: 0.2619 - val_loss: 2.2587 - val_fbeta: 0.0347\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.25670\n",
      "Epoch 95/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.8284 - fbeta: 0.2674 - val_loss: 2.2364 - val_fbeta: 0.0766\n",
      "\n",
      "Epoch 00095: val_loss improved from 2.25670 to 2.23645, saving model to model/model.w.best.h5\n",
      "Epoch 96/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7753 - fbeta: 0.2917 - val_loss: 2.4088 - val_fbeta: 0.0534\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.23645\n",
      "Epoch 97/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7730 - fbeta: 0.2820 - val_loss: 2.2379 - val_fbeta: 0.0992\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.23645\n",
      "Epoch 98/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7188 - fbeta: 0.3005 - val_loss: 2.3288 - val_fbeta: 0.0538\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.23645\n",
      "Epoch 99/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7156 - fbeta: 0.2985 - val_loss: 2.1829 - val_fbeta: 0.0884\n",
      "\n",
      "Epoch 00099: val_loss improved from 2.23645 to 2.18288, saving model to model/model.w.best.h5\n",
      "Epoch 100/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7156 - fbeta: 0.3239 - val_loss: 2.2375 - val_fbeta: 0.0764\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.18288\n",
      "Epoch 101/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.7020 - fbeta: 0.3135 - val_loss: 2.2368 - val_fbeta: 0.0334\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2.18288\n",
      "Epoch 102/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.5897 - fbeta: 0.3577 - val_loss: 2.1250 - val_fbeta: 0.1491\n",
      "\n",
      "Epoch 00102: val_loss improved from 2.18288 to 2.12496, saving model to model/model.w.best.h5\n",
      "Epoch 103/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.6111 - fbeta: 0.3290 - val_loss: 2.1779 - val_fbeta: 0.0871\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2.12496\n",
      "Epoch 104/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.5433 - fbeta: 0.4047 - val_loss: 2.1935 - val_fbeta: 0.1331\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2.12496\n",
      "Epoch 105/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.5618 - fbeta: 0.3786 - val_loss: 2.1141 - val_fbeta: 0.1087\n",
      "\n",
      "Epoch 00105: val_loss improved from 2.12496 to 2.11415, saving model to model/model.w.best.h5\n",
      "Epoch 106/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.4884 - fbeta: 0.4117 - val_loss: 2.1132 - val_fbeta: 0.1173\n",
      "\n",
      "Epoch 00106: val_loss improved from 2.11415 to 2.11318, saving model to model/model.w.best.h5\n",
      "Epoch 107/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.4882 - fbeta: 0.3872 - val_loss: 2.1469 - val_fbeta: 0.1944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00107: val_loss did not improve from 2.11318\n",
      "Epoch 108/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.4345 - fbeta: 0.4538 - val_loss: 2.1909 - val_fbeta: 0.1634\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 2.11318\n",
      "Epoch 109/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.3995 - fbeta: 0.4667 - val_loss: 2.0959 - val_fbeta: 0.1650\n",
      "\n",
      "Epoch 00109: val_loss improved from 2.11318 to 2.09594, saving model to model/model.w.best.h5\n",
      "Epoch 110/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.3460 - fbeta: 0.5140 - val_loss: 2.0510 - val_fbeta: 0.1951\n",
      "\n",
      "Epoch 00110: val_loss improved from 2.09594 to 2.05101, saving model to model/model.w.best.h5\n",
      "Epoch 111/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.3870 - fbeta: 0.5016 - val_loss: 2.1429 - val_fbeta: 0.2325\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 2.05101\n",
      "Epoch 112/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.4046 - fbeta: 0.4801 - val_loss: 2.0262 - val_fbeta: 0.1844\n",
      "\n",
      "Epoch 00112: val_loss improved from 2.05101 to 2.02618, saving model to model/model.w.best.h5\n",
      "Epoch 113/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.3071 - fbeta: 0.4976 - val_loss: 2.0673 - val_fbeta: 0.2119\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 2.02618\n",
      "Epoch 114/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.2466 - fbeta: 0.5479 - val_loss: 2.0240 - val_fbeta: 0.2255\n",
      "\n",
      "Epoch 00114: val_loss improved from 2.02618 to 2.02400, saving model to model/model.w.best.h5\n",
      "Epoch 115/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.2518 - fbeta: 0.5414 - val_loss: 2.0175 - val_fbeta: 0.2484\n",
      "\n",
      "Epoch 00115: val_loss improved from 2.02400 to 2.01748, saving model to model/model.w.best.h5\n",
      "Epoch 116/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.2123 - fbeta: 0.5525 - val_loss: 1.9406 - val_fbeta: 0.2346\n",
      "\n",
      "Epoch 00116: val_loss improved from 2.01748 to 1.94061, saving model to model/model.w.best.h5\n",
      "Epoch 117/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.1795 - fbeta: 0.5625 - val_loss: 1.9136 - val_fbeta: 0.2427\n",
      "\n",
      "Epoch 00117: val_loss improved from 1.94061 to 1.91357, saving model to model/model.w.best.h5\n",
      "Epoch 118/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.1690 - fbeta: 0.5831 - val_loss: 1.9069 - val_fbeta: 0.2951\n",
      "\n",
      "Epoch 00118: val_loss improved from 1.91357 to 1.90688, saving model to model/model.w.best.h5\n",
      "Epoch 119/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 1.1827 - fbeta: 0.5695 - val_loss: 1.9503 - val_fbeta: 0.2899\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.90688\n",
      "Epoch 120/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.1186 - fbeta: 0.5917 - val_loss: 2.0377 - val_fbeta: 0.2203\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.90688\n",
      "Epoch 121/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 1.0772 - fbeta: 0.6142 - val_loss: 1.8828 - val_fbeta: 0.3494\n",
      "\n",
      "Epoch 00121: val_loss improved from 1.90688 to 1.88281, saving model to model/model.w.best.h5\n",
      "Epoch 122/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.9809 - fbeta: 0.6579 - val_loss: 1.9072 - val_fbeta: 0.3693\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.88281\n",
      "Epoch 123/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.0384 - fbeta: 0.6383 - val_loss: 1.9073 - val_fbeta: 0.3455\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.88281\n",
      "Epoch 124/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.0358 - fbeta: 0.6307 - val_loss: 1.9400 - val_fbeta: 0.3392\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.88281\n",
      "Epoch 125/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.9939 - fbeta: 0.6684 - val_loss: 1.8201 - val_fbeta: 0.3883\n",
      "\n",
      "Epoch 00125: val_loss improved from 1.88281 to 1.82009, saving model to model/model.w.best.h5\n",
      "Epoch 126/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 1.0093 - fbeta: 0.6637 - val_loss: 1.8523 - val_fbeta: 0.3792\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.82009\n",
      "Epoch 127/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.9492 - fbeta: 0.6710 - val_loss: 1.7231 - val_fbeta: 0.3989\n",
      "\n",
      "Epoch 00127: val_loss improved from 1.82009 to 1.72308, saving model to model/model.w.best.h5\n",
      "Epoch 128/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.9314 - fbeta: 0.6758 - val_loss: 1.7685 - val_fbeta: 0.4045\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.72308\n",
      "Epoch 129/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.8919 - fbeta: 0.6950 - val_loss: 1.6772 - val_fbeta: 0.4111\n",
      "\n",
      "Epoch 00129: val_loss improved from 1.72308 to 1.67720, saving model to model/model.w.best.h5\n",
      "Epoch 130/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.8554 - fbeta: 0.7073 - val_loss: 1.6912 - val_fbeta: 0.4422\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.67720\n",
      "Epoch 131/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.8508 - fbeta: 0.6954 - val_loss: 1.6632 - val_fbeta: 0.4483\n",
      "\n",
      "Epoch 00131: val_loss improved from 1.67720 to 1.66324, saving model to model/model.w.best.h5\n",
      "Epoch 132/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.8275 - fbeta: 0.7129 - val_loss: 1.7165 - val_fbeta: 0.4225\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.66324\n",
      "Epoch 133/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.8269 - fbeta: 0.7212 - val_loss: 1.6293 - val_fbeta: 0.4326\n",
      "\n",
      "Epoch 00133: val_loss improved from 1.66324 to 1.62934, saving model to model/model.w.best.h5\n",
      "Epoch 134/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.7310 - fbeta: 0.7572 - val_loss: 1.6097 - val_fbeta: 0.4662\n",
      "\n",
      "Epoch 00134: val_loss improved from 1.62934 to 1.60972, saving model to model/model.w.best.h5\n",
      "Epoch 135/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.7563 - fbeta: 0.7455 - val_loss: 1.6002 - val_fbeta: 0.4481\n",
      "\n",
      "Epoch 00135: val_loss improved from 1.60972 to 1.60024, saving model to model/model.w.best.h5\n",
      "Epoch 136/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6990 - fbeta: 0.7815 - val_loss: 1.5579 - val_fbeta: 0.4849\n",
      "\n",
      "Epoch 00136: val_loss improved from 1.60024 to 1.55792, saving model to model/model.w.best.h5\n",
      "Epoch 137/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6870 - fbeta: 0.7651 - val_loss: 1.5640 - val_fbeta: 0.5045\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.55792\n",
      "Epoch 138/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6113 - fbeta: 0.8085 - val_loss: 1.6809 - val_fbeta: 0.5208\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.55792\n",
      "Epoch 139/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6699 - fbeta: 0.7899 - val_loss: 1.4915 - val_fbeta: 0.5111\n",
      "\n",
      "Epoch 00139: val_loss improved from 1.55792 to 1.49151, saving model to model/model.w.best.h5\n",
      "Epoch 140/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6265 - fbeta: 0.7919 - val_loss: 1.5765 - val_fbeta: 0.4736\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.49151\n",
      "Epoch 141/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.6008 - fbeta: 0.7995 - val_loss: 1.5179 - val_fbeta: 0.5197\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.49151\n",
      "Epoch 142/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5823 - fbeta: 0.8219 - val_loss: 1.4938 - val_fbeta: 0.5477\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.49151\n",
      "Epoch 143/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5692 - fbeta: 0.8266 - val_loss: 1.5162 - val_fbeta: 0.4828\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.49151\n",
      "Epoch 144/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5546 - fbeta: 0.8432 - val_loss: 1.4566 - val_fbeta: 0.5874\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.49151 to 1.45656, saving model to model/model.w.best.h5\n",
      "Epoch 145/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5013 - fbeta: 0.8489 - val_loss: 1.4989 - val_fbeta: 0.5490\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.45656\n",
      "Epoch 146/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5471 - fbeta: 0.8254 - val_loss: 1.4211 - val_fbeta: 0.5502\n",
      "\n",
      "Epoch 00146: val_loss improved from 1.45656 to 1.42113, saving model to model/model.w.best.h5\n",
      "Epoch 147/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5093 - fbeta: 0.8248 - val_loss: 1.4063 - val_fbeta: 0.5933\n",
      "\n",
      "Epoch 00147: val_loss improved from 1.42113 to 1.40635, saving model to model/model.w.best.h5\n",
      "Epoch 148/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.5330 - fbeta: 0.8274 - val_loss: 1.4650 - val_fbeta: 0.5685\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.40635\n",
      "Epoch 149/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.5385 - fbeta: 0.8392 - val_loss: 1.5186 - val_fbeta: 0.5542\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.40635\n",
      "Epoch 150/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4996 - fbeta: 0.8420 - val_loss: 1.4454 - val_fbeta: 0.5507\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.40635\n",
      "Epoch 151/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4922 - fbeta: 0.8541 - val_loss: 1.3860 - val_fbeta: 0.6024\n",
      "\n",
      "Epoch 00151: val_loss improved from 1.40635 to 1.38600, saving model to model/model.w.best.h5\n",
      "Epoch 152/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4665 - fbeta: 0.8518 - val_loss: 1.4355 - val_fbeta: 0.5791\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.38600\n",
      "Epoch 153/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3992 - fbeta: 0.8795 - val_loss: 1.3359 - val_fbeta: 0.6432\n",
      "\n",
      "Epoch 00153: val_loss improved from 1.38600 to 1.33591, saving model to model/model.w.best.h5\n",
      "Epoch 154/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4136 - fbeta: 0.8768 - val_loss: 1.3473 - val_fbeta: 0.6286\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.33591\n",
      "Epoch 155/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4042 - fbeta: 0.8744 - val_loss: 1.3471 - val_fbeta: 0.5747\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.33591\n",
      "Epoch 156/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4231 - fbeta: 0.8685 - val_loss: 1.3549 - val_fbeta: 0.6326\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.33591\n",
      "Epoch 157/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.4268 - fbeta: 0.8609 - val_loss: 1.4272 - val_fbeta: 0.5791\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.33591\n",
      "Epoch 158/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.4028 - fbeta: 0.8755 - val_loss: 1.3175 - val_fbeta: 0.6370\n",
      "\n",
      "Epoch 00158: val_loss improved from 1.33591 to 1.31754, saving model to model/model.w.best.h5\n",
      "Epoch 159/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3781 - fbeta: 0.8810 - val_loss: 1.3631 - val_fbeta: 0.6098\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.31754\n",
      "Epoch 160/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3593 - fbeta: 0.8917 - val_loss: 1.4420 - val_fbeta: 0.5996\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.31754\n",
      "Epoch 161/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3340 - fbeta: 0.8959 - val_loss: 1.3058 - val_fbeta: 0.6217\n",
      "\n",
      "Epoch 00161: val_loss improved from 1.31754 to 1.30580, saving model to model/model.w.best.h5\n",
      "Epoch 162/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3326 - fbeta: 0.9011 - val_loss: 1.3573 - val_fbeta: 0.6323\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.30580\n",
      "Epoch 163/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3224 - fbeta: 0.9047 - val_loss: 1.2712 - val_fbeta: 0.6483\n",
      "\n",
      "Epoch 00163: val_loss improved from 1.30580 to 1.27123, saving model to model/model.w.best.h5\n",
      "Epoch 164/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3332 - fbeta: 0.8938 - val_loss: 1.2763 - val_fbeta: 0.6495\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.27123\n",
      "Epoch 165/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2880 - fbeta: 0.9218 - val_loss: 1.3468 - val_fbeta: 0.6312\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.27123\n",
      "Epoch 166/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2906 - fbeta: 0.9169 - val_loss: 1.3224 - val_fbeta: 0.6297\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.27123\n",
      "Epoch 167/500\n",
      "768/768 [==============================] - 27s 36ms/step - loss: 0.2865 - fbeta: 0.9172 - val_loss: 1.3759 - val_fbeta: 0.6347\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.27123\n",
      "Epoch 168/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3097 - fbeta: 0.9034 - val_loss: 1.3105 - val_fbeta: 0.6487\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.27123\n",
      "Epoch 169/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.3138 - fbeta: 0.9157 - val_loss: 1.2725 - val_fbeta: 0.6558\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.27123\n",
      "Epoch 170/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2747 - fbeta: 0.9195 - val_loss: 1.2776 - val_fbeta: 0.6615\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.27123\n",
      "Epoch 171/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2475 - fbeta: 0.9348 - val_loss: 1.2759 - val_fbeta: 0.6638\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.27123\n",
      "Epoch 172/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2406 - fbeta: 0.9314 - val_loss: 1.2474 - val_fbeta: 0.6886\n",
      "\n",
      "Epoch 00172: val_loss improved from 1.27123 to 1.24744, saving model to model/model.w.best.h5\n",
      "Epoch 173/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2503 - fbeta: 0.9332 - val_loss: 1.1687 - val_fbeta: 0.6622\n",
      "\n",
      "Epoch 00173: val_loss improved from 1.24744 to 1.16867, saving model to model/model.w.best.h5\n",
      "Epoch 174/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2233 - fbeta: 0.9393 - val_loss: 1.2283 - val_fbeta: 0.6725\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.16867\n",
      "Epoch 175/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2048 - fbeta: 0.9494 - val_loss: 1.2447 - val_fbeta: 0.6668\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.16867\n",
      "Epoch 176/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1910 - fbeta: 0.9501 - val_loss: 1.1597 - val_fbeta: 0.6804\n",
      "\n",
      "Epoch 00176: val_loss improved from 1.16867 to 1.15970, saving model to model/model.w.best.h5\n",
      "Epoch 177/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2002 - fbeta: 0.9426 - val_loss: 1.2799 - val_fbeta: 0.6727\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.15970\n",
      "Epoch 178/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2000 - fbeta: 0.9470 - val_loss: 1.1641 - val_fbeta: 0.6782\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.15970\n",
      "Epoch 179/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1938 - fbeta: 0.9537 - val_loss: 1.2455 - val_fbeta: 0.6580\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.15970\n",
      "Epoch 180/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.2160 - fbeta: 0.9457 - val_loss: 1.3291 - val_fbeta: 0.6607\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.15970\n",
      "Epoch 181/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1908 - fbeta: 0.9485 - val_loss: 1.1862 - val_fbeta: 0.7138\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.15970\n",
      "Epoch 182/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1930 - fbeta: 0.9518 - val_loss: 1.2625 - val_fbeta: 0.6591\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.15970\n",
      "Epoch 183/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1828 - fbeta: 0.9544 - val_loss: 1.0813 - val_fbeta: 0.7077\n",
      "\n",
      "Epoch 00183: val_loss improved from 1.15970 to 1.08133, saving model to model/model.w.best.h5\n",
      "Epoch 184/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1781 - fbeta: 0.9525 - val_loss: 1.1422 - val_fbeta: 0.6790\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.08133\n",
      "Epoch 185/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1825 - fbeta: 0.9536 - val_loss: 1.1632 - val_fbeta: 0.6866\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.08133\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1572 - fbeta: 0.9654 - val_loss: 1.1403 - val_fbeta: 0.6963\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.08133\n",
      "Epoch 187/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1678 - fbeta: 0.9584 - val_loss: 1.2453 - val_fbeta: 0.6805\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.08133\n",
      "Epoch 188/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1831 - fbeta: 0.9549 - val_loss: 1.1594 - val_fbeta: 0.7067\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.08133\n",
      "Epoch 189/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1527 - fbeta: 0.9654 - val_loss: 1.0900 - val_fbeta: 0.7016\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.08133\n",
      "Epoch 190/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1700 - fbeta: 0.9607 - val_loss: 1.0949 - val_fbeta: 0.7044\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.08133\n",
      "Epoch 191/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1644 - fbeta: 0.9571 - val_loss: 1.2177 - val_fbeta: 0.6820\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.08133\n",
      "Epoch 192/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1571 - fbeta: 0.9568 - val_loss: 1.1288 - val_fbeta: 0.6987\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.08133\n",
      "Epoch 193/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1524 - fbeta: 0.9641 - val_loss: 1.1669 - val_fbeta: 0.6869\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.08133\n",
      "Epoch 194/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1393 - fbeta: 0.9695 - val_loss: 1.3380 - val_fbeta: 0.6902\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.08133\n",
      "Epoch 195/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1477 - fbeta: 0.9607 - val_loss: 1.1056 - val_fbeta: 0.6767\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.08133\n",
      "Epoch 196/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1428 - fbeta: 0.9662 - val_loss: 1.2662 - val_fbeta: 0.6763\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.08133\n",
      "Epoch 197/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1453 - fbeta: 0.9600 - val_loss: 1.1715 - val_fbeta: 0.6820\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.08133\n",
      "Epoch 198/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1266 - fbeta: 0.9722 - val_loss: 1.0883 - val_fbeta: 0.6883\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.08133\n",
      "Epoch 199/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1072 - fbeta: 0.9743 - val_loss: 1.1687 - val_fbeta: 0.6941\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.08133\n",
      "Epoch 200/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1242 - fbeta: 0.9648 - val_loss: 1.2495 - val_fbeta: 0.6550\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.08133\n",
      "Epoch 201/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1402 - fbeta: 0.9572 - val_loss: 1.1345 - val_fbeta: 0.6900\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.08133\n",
      "Epoch 202/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1143 - fbeta: 0.9709 - val_loss: 1.0847 - val_fbeta: 0.7265\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.08133\n",
      "Epoch 203/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1246 - fbeta: 0.9683 - val_loss: 1.2513 - val_fbeta: 0.6578\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.08133\n",
      "Epoch 204/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1270 - fbeta: 0.9642 - val_loss: 1.1012 - val_fbeta: 0.7122\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.08133\n",
      "Epoch 205/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1177 - fbeta: 0.9715 - val_loss: 1.2116 - val_fbeta: 0.6466\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.08133\n",
      "Epoch 206/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1045 - fbeta: 0.9709 - val_loss: 1.0366 - val_fbeta: 0.7047\n",
      "\n",
      "Epoch 00206: val_loss improved from 1.08133 to 1.03664, saving model to model/model.w.best.h5\n",
      "Epoch 207/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1003 - fbeta: 0.9781 - val_loss: 1.1484 - val_fbeta: 0.6804\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.03664\n",
      "Epoch 208/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0822 - fbeta: 0.9850 - val_loss: 1.1301 - val_fbeta: 0.6826\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.03664\n",
      "Epoch 209/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0969 - fbeta: 0.9743 - val_loss: 1.1168 - val_fbeta: 0.6904\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.03664\n",
      "Epoch 210/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1064 - fbeta: 0.9776 - val_loss: 1.0993 - val_fbeta: 0.6720\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.03664\n",
      "Epoch 211/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0967 - fbeta: 0.9829 - val_loss: 1.1329 - val_fbeta: 0.6812\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.03664\n",
      "Epoch 212/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0863 - fbeta: 0.9769 - val_loss: 1.0682 - val_fbeta: 0.6942\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.03664\n",
      "Epoch 213/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0881 - fbeta: 0.9809 - val_loss: 1.1171 - val_fbeta: 0.6974\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.03664\n",
      "Epoch 214/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.1048 - fbeta: 0.9750 - val_loss: 1.2389 - val_fbeta: 0.6728\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.03664\n",
      "Epoch 215/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.1019 - fbeta: 0.9783 - val_loss: 1.0968 - val_fbeta: 0.7134\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.03664\n",
      "Epoch 216/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0945 - fbeta: 0.9814 - val_loss: 1.0747 - val_fbeta: 0.6875\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.03664\n",
      "Epoch 217/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0700 - fbeta: 0.9902 - val_loss: 1.1342 - val_fbeta: 0.6993\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.03664\n",
      "Epoch 218/500\n",
      "768/768 [==============================] - 26s 35ms/step - loss: 0.0879 - fbeta: 0.9791 - val_loss: 1.1420 - val_fbeta: 0.6989\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.03664\n",
      "Epoch 219/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0915 - fbeta: 0.9756 - val_loss: 1.1490 - val_fbeta: 0.6985\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.03664\n",
      "Epoch 220/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0895 - fbeta: 0.9783 - val_loss: 1.1536 - val_fbeta: 0.6723\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.03664\n",
      "Epoch 221/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0686 - fbeta: 0.9889 - val_loss: 1.1098 - val_fbeta: 0.7154\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.03664\n",
      "Epoch 222/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0889 - fbeta: 0.9783 - val_loss: 1.1012 - val_fbeta: 0.6863\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.03664\n",
      "Epoch 223/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0689 - fbeta: 0.9836 - val_loss: 1.1192 - val_fbeta: 0.6959\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.03664\n",
      "Epoch 224/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0684 - fbeta: 0.9823 - val_loss: 1.0294 - val_fbeta: 0.7177\n",
      "\n",
      "Epoch 00224: val_loss improved from 1.03664 to 1.02945, saving model to model/model.w.best.h5\n",
      "Epoch 225/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0849 - fbeta: 0.9782 - val_loss: 1.1926 - val_fbeta: 0.6831\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.02945\n",
      "Epoch 226/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0769 - fbeta: 0.9869 - val_loss: 1.0807 - val_fbeta: 0.7185\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.02945\n",
      "Epoch 227/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0717 - fbeta: 0.9868 - val_loss: 1.1277 - val_fbeta: 0.6921\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.02945\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0687 - fbeta: 0.9836 - val_loss: 1.0648 - val_fbeta: 0.6715\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.02945\n",
      "Epoch 229/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0567 - fbeta: 0.9889 - val_loss: 1.1413 - val_fbeta: 0.6961\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.02945\n",
      "Epoch 230/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0636 - fbeta: 0.9894 - val_loss: 1.0138 - val_fbeta: 0.7188\n",
      "\n",
      "Epoch 00230: val_loss improved from 1.02945 to 1.01383, saving model to model/model.w.best.h5\n",
      "Epoch 231/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0756 - fbeta: 0.9816 - val_loss: 1.2093 - val_fbeta: 0.6792\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.01383\n",
      "Epoch 232/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0641 - fbeta: 0.9888 - val_loss: 1.1820 - val_fbeta: 0.7089\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.01383\n",
      "Epoch 233/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0731 - fbeta: 0.9803 - val_loss: 1.0301 - val_fbeta: 0.7017\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.01383\n",
      "Epoch 234/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0795 - fbeta: 0.9784 - val_loss: 1.1635 - val_fbeta: 0.6778\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.01383\n",
      "Epoch 235/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0890 - fbeta: 0.9750 - val_loss: 1.0756 - val_fbeta: 0.7210\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.01383\n",
      "Epoch 236/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0838 - fbeta: 0.9783 - val_loss: 1.1627 - val_fbeta: 0.6880\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.01383\n",
      "Epoch 237/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0710 - fbeta: 0.9849 - val_loss: 1.0580 - val_fbeta: 0.6939\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.01383\n",
      "Epoch 238/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0618 - fbeta: 0.9842 - val_loss: 1.1291 - val_fbeta: 0.7100\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.01383\n",
      "Epoch 239/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0715 - fbeta: 0.9842 - val_loss: 1.1569 - val_fbeta: 0.6972\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.01383\n",
      "Epoch 240/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0615 - fbeta: 0.9889 - val_loss: 1.0595 - val_fbeta: 0.6864\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.01383\n",
      "Epoch 241/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0572 - fbeta: 0.9882 - val_loss: 1.1559 - val_fbeta: 0.6884\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.01383\n",
      "Epoch 242/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0559 - fbeta: 0.9888 - val_loss: 1.1003 - val_fbeta: 0.6969\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.01383\n",
      "Epoch 243/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0558 - fbeta: 0.9856 - val_loss: 1.0717 - val_fbeta: 0.7076\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.01383\n",
      "Epoch 244/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0577 - fbeta: 0.9849 - val_loss: 1.0939 - val_fbeta: 0.6900\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.01383\n",
      "Epoch 245/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0467 - fbeta: 0.9915 - val_loss: 1.0916 - val_fbeta: 0.7078\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.01383\n",
      "Epoch 246/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0497 - fbeta: 0.9915 - val_loss: 1.2327 - val_fbeta: 0.6856\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.01383\n",
      "Epoch 247/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0508 - fbeta: 0.9875 - val_loss: 1.0552 - val_fbeta: 0.7025\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.01383\n",
      "Epoch 248/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0488 - fbeta: 0.9901 - val_loss: 1.0978 - val_fbeta: 0.7309\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.01383\n",
      "Epoch 249/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0430 - fbeta: 0.9954 - val_loss: 1.1707 - val_fbeta: 0.6942\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.01383\n",
      "Epoch 250/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0580 - fbeta: 0.9850 - val_loss: 1.0414 - val_fbeta: 0.7203\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.01383\n",
      "Epoch 251/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0504 - fbeta: 0.9915 - val_loss: 1.2066 - val_fbeta: 0.6739\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.01383\n",
      "Epoch 252/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0513 - fbeta: 0.9908 - val_loss: 1.0303 - val_fbeta: 0.7045\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.01383\n",
      "Epoch 253/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0508 - fbeta: 0.9889 - val_loss: 1.1156 - val_fbeta: 0.6992\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.01383\n",
      "Epoch 254/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0533 - fbeta: 0.9928 - val_loss: 1.1656 - val_fbeta: 0.6896\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.01383\n",
      "Epoch 255/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0573 - fbeta: 0.9836 - val_loss: 1.1496 - val_fbeta: 0.6737\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.01383\n",
      "Epoch 256/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0769 - fbeta: 0.9850 - val_loss: 1.0518 - val_fbeta: 0.7043\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.01383\n",
      "Epoch 257/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0548 - fbeta: 0.9921 - val_loss: 1.2832 - val_fbeta: 0.6816\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.01383\n",
      "Epoch 258/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0449 - fbeta: 0.9928 - val_loss: 1.0151 - val_fbeta: 0.7268\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.01383\n",
      "Epoch 259/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0521 - fbeta: 0.9895 - val_loss: 1.0905 - val_fbeta: 0.7075\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.01383\n",
      "Epoch 260/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0467 - fbeta: 0.9915 - val_loss: 1.1453 - val_fbeta: 0.6859\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.01383\n",
      "Epoch 261/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0471 - fbeta: 0.9921 - val_loss: 1.0532 - val_fbeta: 0.7109\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.01383\n",
      "Epoch 262/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0457 - fbeta: 0.9908 - val_loss: 1.1003 - val_fbeta: 0.7137\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.01383\n",
      "Epoch 263/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0412 - fbeta: 0.9941 - val_loss: 1.0980 - val_fbeta: 0.6999\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.01383\n",
      "Epoch 264/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0443 - fbeta: 0.9941 - val_loss: 1.1283 - val_fbeta: 0.7002\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.01383\n",
      "Epoch 265/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0430 - fbeta: 0.9908 - val_loss: 1.0899 - val_fbeta: 0.7111\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.01383\n",
      "Epoch 266/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0455 - fbeta: 0.9921 - val_loss: 1.1323 - val_fbeta: 0.6849\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.01383\n",
      "Epoch 267/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0342 - fbeta: 0.9935 - val_loss: 1.1415 - val_fbeta: 0.6899\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.01383\n",
      "Epoch 268/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0403 - fbeta: 0.9922 - val_loss: 1.2177 - val_fbeta: 0.6992\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.01383\n",
      "Epoch 269/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0425 - fbeta: 0.9921 - val_loss: 1.1812 - val_fbeta: 0.7114\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.01383\n",
      "Epoch 270/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0506 - fbeta: 0.9882 - val_loss: 1.0431 - val_fbeta: 0.7026\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.01383\n",
      "Epoch 271/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0531 - fbeta: 0.9882 - val_loss: 1.1495 - val_fbeta: 0.7132\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.01383\n",
      "Epoch 272/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0448 - fbeta: 0.9895 - val_loss: 0.9862 - val_fbeta: 0.7239\n",
      "\n",
      "Epoch 00272: val_loss improved from 1.01383 to 0.98623, saving model to model/model.w.best.h5\n",
      "Epoch 273/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0535 - fbeta: 0.9856 - val_loss: 1.2506 - val_fbeta: 0.6716\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.98623\n",
      "Epoch 274/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0432 - fbeta: 0.9948 - val_loss: 1.0649 - val_fbeta: 0.7210\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.98623\n",
      "Epoch 275/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0503 - fbeta: 0.9849 - val_loss: 1.0253 - val_fbeta: 0.7107\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.98623\n",
      "Epoch 276/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0465 - fbeta: 0.9915 - val_loss: 1.0634 - val_fbeta: 0.6993\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.98623\n",
      "Epoch 277/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0511 - fbeta: 0.9869 - val_loss: 1.1170 - val_fbeta: 0.7010\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.98623\n",
      "Epoch 278/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0484 - fbeta: 0.9895 - val_loss: 1.2597 - val_fbeta: 0.6741\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.98623\n",
      "Epoch 279/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0399 - fbeta: 0.9928 - val_loss: 1.1438 - val_fbeta: 0.6813\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.98623\n",
      "Epoch 280/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0543 - fbeta: 0.9875 - val_loss: 1.1260 - val_fbeta: 0.7125\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.98623\n",
      "Epoch 281/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0507 - fbeta: 0.9889 - val_loss: 1.0779 - val_fbeta: 0.6946\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.98623\n",
      "Epoch 282/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0401 - fbeta: 0.9922 - val_loss: 1.0744 - val_fbeta: 0.6985\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.98623\n",
      "Epoch 283/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0465 - fbeta: 0.9908 - val_loss: 1.0836 - val_fbeta: 0.7214\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.98623\n",
      "Epoch 284/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0475 - fbeta: 0.9863 - val_loss: 1.1537 - val_fbeta: 0.6847\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.98623\n",
      "Epoch 285/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0337 - fbeta: 0.9967 - val_loss: 1.2122 - val_fbeta: 0.7023\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.98623\n",
      "Epoch 286/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0291 - fbeta: 0.9974 - val_loss: 1.1565 - val_fbeta: 0.6905\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.98623\n",
      "Epoch 287/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0485 - fbeta: 0.9876 - val_loss: 1.2160 - val_fbeta: 0.6943\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.98623\n",
      "Epoch 288/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0410 - fbeta: 0.9901 - val_loss: 1.1021 - val_fbeta: 0.7293\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.98623\n",
      "Epoch 289/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0477 - fbeta: 0.9915 - val_loss: 1.2332 - val_fbeta: 0.6874\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.98623\n",
      "Epoch 290/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0375 - fbeta: 0.9915 - val_loss: 1.1900 - val_fbeta: 0.6855\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.98623\n",
      "Epoch 291/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0360 - fbeta: 0.9915 - val_loss: 1.1055 - val_fbeta: 0.7035\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.98623\n",
      "Epoch 292/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0241 - fbeta: 0.9980 - val_loss: 1.0169 - val_fbeta: 0.7227\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.98623\n",
      "Epoch 293/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0288 - fbeta: 0.9961 - val_loss: 1.1181 - val_fbeta: 0.6899\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.98623\n",
      "Epoch 294/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0333 - fbeta: 0.9928 - val_loss: 1.0537 - val_fbeta: 0.7245\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.98623\n",
      "Epoch 295/500\n",
      "768/768 [==============================] - 26s 35ms/step - loss: 0.0356 - fbeta: 0.9895 - val_loss: 1.1264 - val_fbeta: 0.7024\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.98623\n",
      "Epoch 296/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0277 - fbeta: 0.9948 - val_loss: 1.0709 - val_fbeta: 0.7081\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.98623\n",
      "Epoch 297/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0297 - fbeta: 0.9954 - val_loss: 1.0085 - val_fbeta: 0.7261\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.98623\n",
      "Epoch 298/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0297 - fbeta: 0.9941 - val_loss: 1.0661 - val_fbeta: 0.7206\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.98623\n",
      "Epoch 299/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0274 - fbeta: 0.9941 - val_loss: 1.0816 - val_fbeta: 0.7135\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.98623\n",
      "Epoch 300/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0268 - fbeta: 0.9967 - val_loss: 1.1452 - val_fbeta: 0.6876\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.98623\n",
      "Epoch 301/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0366 - fbeta: 0.9915 - val_loss: 1.0502 - val_fbeta: 0.7036\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.98623\n",
      "Epoch 302/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0329 - fbeta: 0.9934 - val_loss: 1.1531 - val_fbeta: 0.7124\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.98623\n",
      "Epoch 303/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0344 - fbeta: 0.9928 - val_loss: 1.0730 - val_fbeta: 0.7023\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.98623\n",
      "Epoch 304/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0313 - fbeta: 0.9935 - val_loss: 1.1118 - val_fbeta: 0.7238\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.98623\n",
      "Epoch 305/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0390 - fbeta: 0.9921 - val_loss: 1.2228 - val_fbeta: 0.6741\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.98623\n",
      "Epoch 306/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0326 - fbeta: 0.9941 - val_loss: 1.0374 - val_fbeta: 0.7214\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.98623\n",
      "Epoch 307/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0254 - fbeta: 0.9954 - val_loss: 1.1243 - val_fbeta: 0.7296\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.98623\n",
      "Epoch 308/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0331 - fbeta: 0.9935 - val_loss: 1.0885 - val_fbeta: 0.7037\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.98623\n",
      "Epoch 309/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0223 - fbeta: 0.9980 - val_loss: 1.1536 - val_fbeta: 0.6997\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.98623\n",
      "Epoch 310/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0241 - fbeta: 0.9974 - val_loss: 1.1828 - val_fbeta: 0.7100\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.98623\n",
      "Epoch 311/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0296 - fbeta: 0.9948 - val_loss: 1.0015 - val_fbeta: 0.7246\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.98623\n",
      "Epoch 312/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0365 - fbeta: 0.9915 - val_loss: 1.0034 - val_fbeta: 0.7142\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.98623\n",
      "Epoch 313/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0271 - fbeta: 0.9948 - val_loss: 1.1229 - val_fbeta: 0.6855\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.98623\n",
      "Epoch 314/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0233 - fbeta: 0.9974 - val_loss: 1.0562 - val_fbeta: 0.7079\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.98623\n",
      "Epoch 315/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0224 - fbeta: 0.9987 - val_loss: 1.0229 - val_fbeta: 0.7017\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.98623\n",
      "Epoch 316/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0299 - fbeta: 0.9928 - val_loss: 1.0548 - val_fbeta: 0.7056\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.98623\n",
      "Epoch 317/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0264 - fbeta: 0.9967 - val_loss: 1.1263 - val_fbeta: 0.7217\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.98623\n",
      "Epoch 318/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0241 - fbeta: 0.9961 - val_loss: 1.2303 - val_fbeta: 0.6808\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.98623\n",
      "Epoch 319/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0260 - fbeta: 0.9954 - val_loss: 0.9940 - val_fbeta: 0.7528\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.98623\n",
      "Epoch 320/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0259 - fbeta: 0.9954 - val_loss: 1.0125 - val_fbeta: 0.7132\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.98623\n",
      "Epoch 321/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0226 - fbeta: 0.9954 - val_loss: 1.1814 - val_fbeta: 0.7125\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.98623\n",
      "Epoch 322/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0199 - fbeta: 0.9987 - val_loss: 1.1298 - val_fbeta: 0.7083\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.98623\n",
      "Epoch 323/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0375 - fbeta: 0.9915 - val_loss: 1.1021 - val_fbeta: 0.7285\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.98623\n",
      "Epoch 324/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0339 - fbeta: 0.9935 - val_loss: 1.0515 - val_fbeta: 0.7405\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.98623\n",
      "Epoch 325/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0303 - fbeta: 0.9915 - val_loss: 1.0492 - val_fbeta: 0.7255\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.98623\n",
      "Epoch 326/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0282 - fbeta: 0.9922 - val_loss: 1.2196 - val_fbeta: 0.6918\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.98623\n",
      "Epoch 327/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0258 - fbeta: 0.9961 - val_loss: 1.0972 - val_fbeta: 0.7130\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.98623\n",
      "Epoch 328/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0251 - fbeta: 0.9961 - val_loss: 1.0431 - val_fbeta: 0.7273\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.98623\n",
      "Epoch 329/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0284 - fbeta: 0.9954 - val_loss: 1.0226 - val_fbeta: 0.7280\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.98623\n",
      "Epoch 330/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0261 - fbeta: 0.9941 - val_loss: 1.0453 - val_fbeta: 0.6979\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.98623\n",
      "Epoch 331/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0250 - fbeta: 0.9922 - val_loss: 1.1291 - val_fbeta: 0.7020\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.98623\n",
      "Epoch 332/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0193 - fbeta: 0.9980 - val_loss: 1.1454 - val_fbeta: 0.6924\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.98623\n",
      "Epoch 333/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0257 - fbeta: 0.9948 - val_loss: 1.1640 - val_fbeta: 0.7098\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.98623\n",
      "Epoch 334/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0292 - fbeta: 0.9922 - val_loss: 1.1402 - val_fbeta: 0.7024\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.98623\n",
      "Epoch 335/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0195 - fbeta: 0.9974 - val_loss: 1.0935 - val_fbeta: 0.7227\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.98623\n",
      "Epoch 336/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0243 - fbeta: 0.9961 - val_loss: 1.1740 - val_fbeta: 0.6954\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.98623\n",
      "Epoch 337/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0254 - fbeta: 0.9948 - val_loss: 1.1252 - val_fbeta: 0.7134\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.98623\n",
      "Epoch 338/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0303 - fbeta: 0.9922 - val_loss: 1.0633 - val_fbeta: 0.7146\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.98623\n",
      "Epoch 339/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0328 - fbeta: 0.9908 - val_loss: 1.2229 - val_fbeta: 0.7184\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.98623\n",
      "Epoch 340/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0402 - fbeta: 0.9889 - val_loss: 1.1033 - val_fbeta: 0.6978\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.98623\n",
      "Epoch 341/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0255 - fbeta: 0.9967 - val_loss: 1.0410 - val_fbeta: 0.6901\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.98623\n",
      "Epoch 342/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0255 - fbeta: 0.9941 - val_loss: 1.0710 - val_fbeta: 0.7078\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.98623\n",
      "Epoch 343/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0231 - fbeta: 0.9948 - val_loss: 1.1190 - val_fbeta: 0.7071\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.98623\n",
      "Epoch 344/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0294 - fbeta: 0.9915 - val_loss: 1.0557 - val_fbeta: 0.7045\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.98623\n",
      "Epoch 345/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0316 - fbeta: 0.9922 - val_loss: 1.0324 - val_fbeta: 0.7201\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.98623\n",
      "Epoch 346/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0311 - fbeta: 0.9908 - val_loss: 1.0484 - val_fbeta: 0.7230\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.98623\n",
      "Epoch 347/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0296 - fbeta: 0.9935 - val_loss: 1.0841 - val_fbeta: 0.7263\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.98623\n",
      "Epoch 348/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0346 - fbeta: 0.9941 - val_loss: 1.1114 - val_fbeta: 0.6939\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.98623\n",
      "Epoch 349/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0283 - fbeta: 0.9974 - val_loss: 1.0336 - val_fbeta: 0.7155\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.98623\n",
      "Epoch 350/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0183 - fbeta: 0.9993 - val_loss: 0.9410 - val_fbeta: 0.7410\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.98623 to 0.94097, saving model to model/model.w.best.h5\n",
      "Epoch 351/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0207 - fbeta: 0.9967 - val_loss: 1.0976 - val_fbeta: 0.7289\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.94097\n",
      "Epoch 352/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0225 - fbeta: 0.9934 - val_loss: 1.1412 - val_fbeta: 0.6838\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.94097\n",
      "Epoch 353/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0220 - fbeta: 0.9935 - val_loss: 1.0216 - val_fbeta: 0.7245\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.94097\n",
      "Epoch 354/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0244 - fbeta: 0.9961 - val_loss: 1.1102 - val_fbeta: 0.7060\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.94097\n",
      "Epoch 355/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0182 - fbeta: 0.9980 - val_loss: 1.1282 - val_fbeta: 0.7104\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.94097\n",
      "Epoch 356/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0188 - fbeta: 0.9987 - val_loss: 1.1493 - val_fbeta: 0.7234\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.94097\n",
      "Epoch 357/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0271 - fbeta: 0.9922 - val_loss: 1.0808 - val_fbeta: 0.7036\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.94097\n",
      "Epoch 358/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0233 - fbeta: 0.9961 - val_loss: 1.0208 - val_fbeta: 0.7294\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.94097\n",
      "Epoch 359/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0226 - fbeta: 0.9961 - val_loss: 1.1545 - val_fbeta: 0.7141\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.94097\n",
      "Epoch 360/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0293 - fbeta: 0.9954 - val_loss: 1.2124 - val_fbeta: 0.7070\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.94097\n",
      "Epoch 361/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0244 - fbeta: 0.9948 - val_loss: 1.1563 - val_fbeta: 0.7146\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.94097\n",
      "Epoch 362/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0224 - fbeta: 0.9954 - val_loss: 1.1621 - val_fbeta: 0.6965\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.94097\n",
      "Epoch 363/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0224 - fbeta: 0.9954 - val_loss: 1.1332 - val_fbeta: 0.7136\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.94097\n",
      "Epoch 364/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0204 - fbeta: 0.9967 - val_loss: 1.0113 - val_fbeta: 0.7148\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.94097\n",
      "Epoch 365/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0186 - fbeta: 0.9974 - val_loss: 1.1995 - val_fbeta: 0.6987\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.94097\n",
      "Epoch 366/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0171 - fbeta: 0.9974 - val_loss: 1.2149 - val_fbeta: 0.6964\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.94097\n",
      "Epoch 367/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0115 - fbeta: 1.0000 - val_loss: 1.0995 - val_fbeta: 0.7030\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.94097\n",
      "Epoch 368/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0168 - fbeta: 0.9980 - val_loss: 1.1162 - val_fbeta: 0.7134\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.94097\n",
      "Epoch 369/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0162 - fbeta: 0.9987 - val_loss: 1.0251 - val_fbeta: 0.7403\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.94097\n",
      "Epoch 370/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0228 - fbeta: 0.9961 - val_loss: 1.0122 - val_fbeta: 0.7331\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.94097\n",
      "Epoch 371/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0234 - fbeta: 0.9961 - val_loss: 1.0799 - val_fbeta: 0.7070\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.94097\n",
      "Epoch 372/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0158 - fbeta: 0.9980 - val_loss: 1.0585 - val_fbeta: 0.7178\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.94097\n",
      "Epoch 373/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0214 - fbeta: 0.9941 - val_loss: 1.0235 - val_fbeta: 0.7315\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.94097\n",
      "Epoch 374/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0209 - fbeta: 0.9954 - val_loss: 1.1225 - val_fbeta: 0.7085\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.94097\n",
      "Epoch 375/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0172 - fbeta: 0.9980 - val_loss: 1.0392 - val_fbeta: 0.7381\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.94097\n",
      "Epoch 376/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0232 - fbeta: 0.9922 - val_loss: 1.1080 - val_fbeta: 0.7296\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.94097\n",
      "Epoch 377/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0244 - fbeta: 0.9954 - val_loss: 1.1159 - val_fbeta: 0.6948\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.94097\n",
      "Epoch 378/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0223 - fbeta: 0.9961 - val_loss: 1.0916 - val_fbeta: 0.7152\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.94097\n",
      "Epoch 379/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0199 - fbeta: 0.9987 - val_loss: 1.0957 - val_fbeta: 0.7145\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.94097\n",
      "Epoch 380/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0164 - fbeta: 0.9987 - val_loss: 1.0535 - val_fbeta: 0.7168\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.94097\n",
      "Epoch 381/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0122 - fbeta: 1.0000 - val_loss: 1.0797 - val_fbeta: 0.7001\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.94097\n",
      "Epoch 382/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0136 - fbeta: 0.9974 - val_loss: 1.1420 - val_fbeta: 0.6880\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.94097\n",
      "Epoch 383/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0162 - fbeta: 0.9974 - val_loss: 1.1149 - val_fbeta: 0.7159\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.94097\n",
      "Epoch 384/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0219 - fbeta: 0.9941 - val_loss: 0.9971 - val_fbeta: 0.7064\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.94097\n",
      "Epoch 385/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0231 - fbeta: 0.9948 - val_loss: 1.0960 - val_fbeta: 0.6916\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.94097\n",
      "Epoch 386/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0255 - fbeta: 0.9961 - val_loss: 1.0911 - val_fbeta: 0.6908\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.94097\n",
      "Epoch 387/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0195 - fbeta: 0.9993 - val_loss: 1.0342 - val_fbeta: 0.7175\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.94097\n",
      "Epoch 388/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0146 - fbeta: 0.9967 - val_loss: 1.0038 - val_fbeta: 0.7096\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.94097\n",
      "Epoch 389/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0183 - fbeta: 0.9967 - val_loss: 0.9680 - val_fbeta: 0.7348\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.94097\n",
      "Epoch 390/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0190 - fbeta: 0.9974 - val_loss: 1.1120 - val_fbeta: 0.7063\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.94097\n",
      "Epoch 391/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0167 - fbeta: 0.9974 - val_loss: 1.1181 - val_fbeta: 0.7106\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.94097\n",
      "Epoch 392/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0169 - fbeta: 0.9980 - val_loss: 1.0234 - val_fbeta: 0.7243\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.94097\n",
      "Epoch 393/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0185 - fbeta: 0.9974 - val_loss: 1.0854 - val_fbeta: 0.7117\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.94097\n",
      "Epoch 394/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0175 - fbeta: 0.9967 - val_loss: 1.2066 - val_fbeta: 0.7137\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.94097\n",
      "Epoch 395/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0180 - fbeta: 0.9961 - val_loss: 1.1035 - val_fbeta: 0.7284\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.94097\n",
      "Epoch 396/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0150 - fbeta: 0.9961 - val_loss: 1.0371 - val_fbeta: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00396: val_loss did not improve from 0.94097\n",
      "Epoch 397/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0138 - fbeta: 0.9974 - val_loss: 1.1848 - val_fbeta: 0.6961\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.94097\n",
      "Epoch 398/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0190 - fbeta: 0.9980 - val_loss: 1.1326 - val_fbeta: 0.7073\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.94097\n",
      "Epoch 399/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0181 - fbeta: 0.9954 - val_loss: 1.0127 - val_fbeta: 0.7047\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.94097\n",
      "Epoch 400/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0182 - fbeta: 0.9967 - val_loss: 1.0732 - val_fbeta: 0.7381\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.94097\n",
      "Epoch 401/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0137 - fbeta: 0.9980 - val_loss: 1.1205 - val_fbeta: 0.7247\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.94097\n",
      "Epoch 402/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0152 - fbeta: 0.9980 - val_loss: 1.1009 - val_fbeta: 0.7163\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.94097\n",
      "Epoch 403/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0191 - fbeta: 0.9961 - val_loss: 1.0645 - val_fbeta: 0.7271\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.94097\n",
      "Epoch 404/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0157 - fbeta: 0.9974 - val_loss: 1.0472 - val_fbeta: 0.7362\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.94097\n",
      "Epoch 405/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0132 - fbeta: 1.0000 - val_loss: 1.0351 - val_fbeta: 0.7219\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.94097\n",
      "Epoch 406/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0144 - fbeta: 0.9980 - val_loss: 1.0842 - val_fbeta: 0.7162\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.94097\n",
      "Epoch 407/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0155 - fbeta: 0.9987 - val_loss: 1.3220 - val_fbeta: 0.6939\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.94097\n",
      "Epoch 408/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0185 - fbeta: 0.9980 - val_loss: 1.0906 - val_fbeta: 0.7368\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.94097\n",
      "Epoch 409/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0189 - fbeta: 0.9961 - val_loss: 1.0718 - val_fbeta: 0.7191\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.94097\n",
      "Epoch 410/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0099 - fbeta: 1.0000 - val_loss: 1.0887 - val_fbeta: 0.7064\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.94097\n",
      "Epoch 411/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0145 - fbeta: 0.9980 - val_loss: 1.1037 - val_fbeta: 0.7029\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.94097\n",
      "Epoch 412/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0132 - fbeta: 0.9993 - val_loss: 1.1475 - val_fbeta: 0.7148\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.94097\n",
      "Epoch 413/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0179 - fbeta: 0.9954 - val_loss: 1.1180 - val_fbeta: 0.7215\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.94097\n",
      "Epoch 414/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0283 - fbeta: 0.9941 - val_loss: 1.1713 - val_fbeta: 0.7148\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.94097\n",
      "Epoch 415/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0192 - fbeta: 0.9948 - val_loss: 1.0642 - val_fbeta: 0.7382\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.94097\n",
      "Epoch 416/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0253 - fbeta: 0.9935 - val_loss: 1.1339 - val_fbeta: 0.7046\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.94097\n",
      "Epoch 417/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0207 - fbeta: 0.9948 - val_loss: 1.0819 - val_fbeta: 0.7420\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.94097\n",
      "Epoch 418/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0240 - fbeta: 0.9935 - val_loss: 1.0848 - val_fbeta: 0.7038\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.94097\n",
      "Epoch 419/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0170 - fbeta: 0.9961 - val_loss: 1.0875 - val_fbeta: 0.7070\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.94097\n",
      "Epoch 420/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0183 - fbeta: 0.9967 - val_loss: 1.1486 - val_fbeta: 0.7093\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.94097\n",
      "Epoch 421/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0189 - fbeta: 0.9967 - val_loss: 1.1161 - val_fbeta: 0.7109\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.94097\n",
      "Epoch 422/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0219 - fbeta: 0.9948 - val_loss: 1.1941 - val_fbeta: 0.7131\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.94097\n",
      "Epoch 423/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0161 - fbeta: 0.9987 - val_loss: 1.0813 - val_fbeta: 0.7266\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.94097\n",
      "Epoch 424/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0132 - fbeta: 0.9987 - val_loss: 1.0883 - val_fbeta: 0.7101\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.94097\n",
      "Epoch 425/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0139 - fbeta: 0.9961 - val_loss: 1.0852 - val_fbeta: 0.6992\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.94097\n",
      "Epoch 426/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0124 - fbeta: 1.0000 - val_loss: 1.0880 - val_fbeta: 0.7246\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.94097\n",
      "Epoch 427/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0171 - fbeta: 0.9987 - val_loss: 1.1634 - val_fbeta: 0.7109\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.94097\n",
      "Epoch 428/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0160 - fbeta: 0.9974 - val_loss: 1.1921 - val_fbeta: 0.7138\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.94097\n",
      "Epoch 429/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0148 - fbeta: 0.9967 - val_loss: 1.1158 - val_fbeta: 0.6976\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.94097\n",
      "Epoch 430/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0182 - fbeta: 0.9954 - val_loss: 1.0420 - val_fbeta: 0.7075\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.94097\n",
      "Epoch 431/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0157 - fbeta: 0.9987 - val_loss: 1.0713 - val_fbeta: 0.6953\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.94097\n",
      "Epoch 432/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0109 - fbeta: 0.9993 - val_loss: 1.1309 - val_fbeta: 0.7007\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.94097\n",
      "Epoch 433/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0122 - fbeta: 0.9987 - val_loss: 1.1080 - val_fbeta: 0.7385\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.94097\n",
      "Epoch 434/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0100 - fbeta: 0.9993 - val_loss: 1.0274 - val_fbeta: 0.7381\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.94097\n",
      "Epoch 435/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0109 - fbeta: 1.0000 - val_loss: 1.0477 - val_fbeta: 0.7083\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.94097\n",
      "Epoch 436/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0135 - fbeta: 0.9987 - val_loss: 1.0997 - val_fbeta: 0.7021\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.94097\n",
      "Epoch 437/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0229 - fbeta: 0.9928 - val_loss: 1.2271 - val_fbeta: 0.6865\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.94097\n",
      "Epoch 438/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0282 - fbeta: 0.9922 - val_loss: 1.1680 - val_fbeta: 0.7067\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.94097\n",
      "Epoch 439/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0117 - fbeta: 1.0000 - val_loss: 1.1464 - val_fbeta: 0.7170\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.94097\n",
      "Epoch 440/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0128 - fbeta: 1.0000 - val_loss: 0.9539 - val_fbeta: 0.7479\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.94097\n",
      "Epoch 441/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0135 - fbeta: 0.9980 - val_loss: 0.9478 - val_fbeta: 0.7373\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.94097\n",
      "Epoch 442/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0174 - fbeta: 0.9974 - val_loss: 1.1065 - val_fbeta: 0.7206\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.94097\n",
      "Epoch 443/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0144 - fbeta: 0.9980 - val_loss: 1.1696 - val_fbeta: 0.7206\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.94097\n",
      "Epoch 444/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0093 - fbeta: 1.0000 - val_loss: 1.0504 - val_fbeta: 0.7392\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.94097\n",
      "Epoch 445/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0167 - fbeta: 0.9961 - val_loss: 1.0986 - val_fbeta: 0.7220\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.94097\n",
      "Epoch 446/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0168 - fbeta: 0.9954 - val_loss: 1.1841 - val_fbeta: 0.7081\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.94097\n",
      "Epoch 447/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0166 - fbeta: 0.9987 - val_loss: 1.0526 - val_fbeta: 0.7190\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.94097\n",
      "Epoch 448/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0212 - fbeta: 0.9948 - val_loss: 1.0743 - val_fbeta: 0.7285\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.94097\n",
      "Epoch 449/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0266 - fbeta: 0.9948 - val_loss: 1.0580 - val_fbeta: 0.7238\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.94097\n",
      "Epoch 450/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0167 - fbeta: 0.9974 - val_loss: 1.0000 - val_fbeta: 0.7093\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.94097\n",
      "Epoch 451/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0169 - fbeta: 0.9961 - val_loss: 1.0873 - val_fbeta: 0.7128\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.94097\n",
      "Epoch 452/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0230 - fbeta: 0.9941 - val_loss: 1.0633 - val_fbeta: 0.7059\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.94097\n",
      "Epoch 453/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0189 - fbeta: 0.9980 - val_loss: 1.1280 - val_fbeta: 0.7303\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.94097\n",
      "Epoch 454/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0128 - fbeta: 0.9993 - val_loss: 1.0866 - val_fbeta: 0.7207\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.94097\n",
      "Epoch 455/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0146 - fbeta: 0.9993 - val_loss: 1.0411 - val_fbeta: 0.7139\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.94097\n",
      "Epoch 456/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0092 - fbeta: 0.9987 - val_loss: 1.0918 - val_fbeta: 0.7108\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.94097\n",
      "Epoch 457/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0189 - fbeta: 0.9967 - val_loss: 1.0712 - val_fbeta: 0.7047\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.94097\n",
      "Epoch 458/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0101 - fbeta: 1.0000 - val_loss: 1.1472 - val_fbeta: 0.7198\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.94097\n",
      "Epoch 459/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0087 - fbeta: 1.0000 - val_loss: 1.1193 - val_fbeta: 0.7172\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.94097\n",
      "Epoch 460/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0154 - fbeta: 0.9974 - val_loss: 1.0687 - val_fbeta: 0.7003\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.94097\n",
      "Epoch 461/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0081 - fbeta: 1.0000 - val_loss: 1.0574 - val_fbeta: 0.7042\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.94097\n",
      "Epoch 462/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0084 - fbeta: 1.0000 - val_loss: 1.1006 - val_fbeta: 0.7012\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.94097\n",
      "Epoch 463/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0093 - fbeta: 0.9993 - val_loss: 1.1208 - val_fbeta: 0.7024\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.94097\n",
      "Epoch 464/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0063 - fbeta: 1.0000 - val_loss: 1.0329 - val_fbeta: 0.7084\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.94097\n",
      "Epoch 465/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0095 - fbeta: 0.9987 - val_loss: 0.9985 - val_fbeta: 0.7313\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.94097\n",
      "Epoch 466/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0145 - fbeta: 0.9948 - val_loss: 1.1775 - val_fbeta: 0.7122\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.94097\n",
      "Epoch 467/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0116 - fbeta: 0.9974 - val_loss: 0.9871 - val_fbeta: 0.7311\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.94097\n",
      "Epoch 468/500\n",
      "768/768 [==============================] - 26s 34ms/step - loss: 0.0107 - fbeta: 0.9974 - val_loss: 1.0746 - val_fbeta: 0.7203\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.94097\n",
      "Epoch 469/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0167 - fbeta: 0.9954 - val_loss: 1.1042 - val_fbeta: 0.6986\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.94097\n",
      "Epoch 470/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0120 - fbeta: 0.9980 - val_loss: 1.1099 - val_fbeta: 0.7234\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.94097\n",
      "Epoch 471/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0093 - fbeta: 0.9993 - val_loss: 1.1516 - val_fbeta: 0.7116\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.94097\n",
      "Epoch 472/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0178 - fbeta: 0.9941 - val_loss: 1.0954 - val_fbeta: 0.7314\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.94097\n",
      "Epoch 473/500\n",
      "768/768 [==============================] - 27s 35ms/step - loss: 0.0154 - fbeta: 0.9967 - val_loss: 1.0533 - val_fbeta: 0.7126\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.94097\n",
      "Epoch 474/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0128 - fbeta: 0.9974 - val_loss: 1.0953 - val_fbeta: 0.6995\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.94097\n",
      "Epoch 475/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0182 - fbeta: 0.9961 - val_loss: 1.0386 - val_fbeta: 0.7249\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.94097\n",
      "Epoch 476/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0114 - fbeta: 0.9980 - val_loss: 1.1133 - val_fbeta: 0.7608\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.94097\n",
      "Epoch 477/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0164 - fbeta: 0.9974 - val_loss: 1.1737 - val_fbeta: 0.7388\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.94097\n",
      "Epoch 478/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0129 - fbeta: 0.9987 - val_loss: 1.0714 - val_fbeta: 0.7103\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.94097\n",
      "Epoch 479/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0103 - fbeta: 1.0000 - val_loss: 0.9518 - val_fbeta: 0.7360\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.94097\n",
      "Epoch 480/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0101 - fbeta: 0.9980 - val_loss: 1.0352 - val_fbeta: 0.7488\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.94097\n",
      "Epoch 481/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0138 - fbeta: 0.9948 - val_loss: 1.2084 - val_fbeta: 0.7159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00481: val_loss did not improve from 0.94097\n",
      "Epoch 482/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0227 - fbeta: 0.9922 - val_loss: 1.0878 - val_fbeta: 0.7303\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.94097\n",
      "Epoch 483/500\n",
      "768/768 [==============================] - 28s 37ms/step - loss: 0.0145 - fbeta: 0.9974 - val_loss: 1.0395 - val_fbeta: 0.7329\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.94097\n",
      "Epoch 484/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0144 - fbeta: 0.9974 - val_loss: 1.0384 - val_fbeta: 0.7089\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.94097\n",
      "Epoch 485/500\n",
      "768/768 [==============================] - 28s 37ms/step - loss: 0.0192 - fbeta: 0.9928 - val_loss: 1.2072 - val_fbeta: 0.7258\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.94097\n",
      "Epoch 486/500\n",
      "768/768 [==============================] - 29s 38ms/step - loss: 0.0140 - fbeta: 0.9974 - val_loss: 1.1356 - val_fbeta: 0.7370\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.94097\n",
      "Epoch 487/500\n",
      "768/768 [==============================] - 28s 37ms/step - loss: 0.0168 - fbeta: 0.9974 - val_loss: 1.0952 - val_fbeta: 0.7239\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.94097\n",
      "Epoch 488/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0174 - fbeta: 0.9961 - val_loss: 1.1157 - val_fbeta: 0.7038\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.94097\n",
      "Epoch 489/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0107 - fbeta: 0.9993 - val_loss: 1.0840 - val_fbeta: 0.7088\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.94097\n",
      "Epoch 490/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0105 - fbeta: 0.9980 - val_loss: 1.0476 - val_fbeta: 0.7400\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.94097\n",
      "Epoch 491/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0136 - fbeta: 0.9974 - val_loss: 1.1699 - val_fbeta: 0.7342\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.94097\n",
      "Epoch 492/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0169 - fbeta: 0.9941 - val_loss: 1.2240 - val_fbeta: 0.7031\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.94097\n",
      "Epoch 493/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0155 - fbeta: 0.9954 - val_loss: 1.0658 - val_fbeta: 0.6967\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.94097\n",
      "Epoch 494/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0103 - fbeta: 0.9974 - val_loss: 1.0632 - val_fbeta: 0.7195\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.94097\n",
      "Epoch 495/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0139 - fbeta: 0.9987 - val_loss: 1.0588 - val_fbeta: 0.7145\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.94097\n",
      "Epoch 496/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0121 - fbeta: 0.9987 - val_loss: 1.1022 - val_fbeta: 0.7090\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.94097\n",
      "Epoch 497/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0114 - fbeta: 0.9987 - val_loss: 1.2659 - val_fbeta: 0.6913\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.94097\n",
      "Epoch 498/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0198 - fbeta: 0.9941 - val_loss: 1.1896 - val_fbeta: 0.7157\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.94097\n",
      "Epoch 499/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0131 - fbeta: 0.9974 - val_loss: 1.0748 - val_fbeta: 0.7331\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.94097\n",
      "Epoch 500/500\n",
      "768/768 [==============================] - 28s 36ms/step - loss: 0.0092 - fbeta: 0.9993 - val_loss: 1.0682 - val_fbeta: 0.7225\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.94097\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import callbacks\n",
    "\n",
    "PARAM_MAX_EPOCHS = 500 # PARAM: number of model-fit runs\n",
    "PARAM_N_BATCH = 64 # PARAM: number of input samples for one feedfwd-backprop step\n",
    "\n",
    "checkpointer = callbacks.ModelCheckpoint (\n",
    "    filepath=os.path.join ('model','model.w.best.h5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit (X_train, y_train,\n",
    "                     epochs=PARAM_MAX_EPOCHS, batch_size=PARAM_N_BATCH, validation_split=(7/39), shuffle=True,\n",
    "                     callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_fbeta', 'val_loss', 'fbeta'])\n"
     ]
    }
   ],
   "source": [
    "print (history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "# summarize history for accuracy\n",
    "axs[0].plot (history.history['fbeta'])\n",
    "if 'val_fbeta' in history.history:\n",
    "    axs[0].plot (history.history['val_fbeta'])\n",
    "axs[0].set (xlabel='epoch', ylabel='score (higher better)', title='F-{} score'.format (PARAM_BETA))\n",
    "axs[0].legend (['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "axs[1].plot (history.history['loss'])\n",
    "if 'val_loss' in history.history:\n",
    "    axs[1].plot (history.history['val_loss'])\n",
    "axs[1].set (xlabel='epoch', ylabel='loss (lower better)', title='loss (cat x-entropy)')\n",
    "axs[1].legend (['train', 'validation'], loc='upper left')\n",
    "\n",
    "fig.set_size_inches ((15., 6.), forward=True)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**discussion**\n",
    "\n",
    "[2018-09-02]\n",
    "\n",
    "epoch 205:\n",
    "- loss: 0.0280\n",
    "- val_loss: 0.4546\n",
    "- fbeta: 1.0000\n",
    "- val_fbeta: 0.9412\n",
    "\n",
    "_\n",
    "\n",
    "Above graphs show the F-beta score per epoch with beta = 1 on the left and the *loss per epoch*, calulated by the mean squared error (mse) on the right.\n",
    "\n",
    "*loss per epoch*:\n",
    "- gradient steps start with a loss of 0.052, end by 0.042 and show a smooth concave curve. The curve couldn't be better except a faster drop in the first 10 epochs.\n",
    "- the worse: mse after 1st epoch = 0.052 - the CNN learns very slow and in tiny steps (1st/2nd epoch: 0.052-0.049 = 0.003)\n",
    "\n",
    "*F-beta score per epoch*\n",
    "- evaluation metric immediately drops to zero after some epochs - the CNN doesn't learn anything yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reasons / todo\n",
    "\n",
    "*input data*\n",
    "\n",
    "(1) The used dataset only has 240 samples for training, validation and test. This is by far nothing for the CNN.\n",
    "\n",
    "Todo: retrieve more samples for the dataset\n",
    "\n",
    "(2) A quick look at random spectrograms show kind of chaotic information - as a human being it is hard to tell if there's any structure behind each key-mode pair. This may apply to the CNN too.\n",
    "\n",
    "Todo: find additional filter techniques / methods to clearly bring out structures for the CNN\n",
    "\n",
    "(3) Songs can change in key over their whole length.\n",
    "\n",
    "Todo: take appropriate sample of a song - ommit bridges, refrains, silent passages, noisy songs\n",
    "\n",
    "_\n",
    "\n",
    "*model training*\n",
    "\n",
    "The model was trained for 100 epochs, each in batches of 10 samples per feedfwd-backprop step. To make sure that the architecture is well suited, more epochs shall be run.\n",
    "\n",
    "Todo: increase epochs, change batch size\n",
    "\n",
    "_\n",
    "\n",
    "*model architecture*\n",
    "\n",
    "Todo: To better understand the insight of the CNN, visualize the filter of the convolutions. May there be enlightenment what kind of architecture works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare learning algorithm to benchmarks\n",
    "\n",
    "[i] below statements can be run without executing the whole notebook\n",
    "\n",
    "Therefor, go to and execute <a href='#load-learning-algorithm'>load learning algorithm</a>\n",
    "\n",
    "**TODO** **TODO** **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**benchmark data - fft**\n",
    "\n",
    "Juypter Notebook <a href='./00.hlp/fft/fft.ipynb#Benchmark-data'>benchmark data - fft</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filenames', 'target', 'target_names', 'DESCR'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD SPECTROGRAM FILENAMES\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "PARAM_RND_STATE = 42\n",
    "\n",
    "container_path = os.path.join ('src_bench', 'src_spectro')\n",
    "load_content = False\n",
    "description = ['Cm', 'C', 'C#m', 'C#',\n",
    "               'Dm', 'D', 'D#m', 'D#',\n",
    "               'Em', 'E',\n",
    "               'Fm', 'F', 'F#m', 'F#',\n",
    "               'Gm', 'G', 'G#m', 'G#',\n",
    "               'Am', 'A', 'A#m', 'A#',\n",
    "               'Bm', 'B']\n",
    "\n",
    "src_bench_spectro_data = datasets.load_files (container_path=container_path,\n",
    "                                              description=description,\n",
    "                                              load_content=load_content,\n",
    "                                              random_state=PARAM_RND_STATE)\n",
    "src_bench_spectro_data.keys ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 390.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "bench_spectro_tensors = paths_to_tensor (src_bench_spectro_data['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 12ms/step\n",
      "[('loss', 4.837411244710286), ('fbeta', 0.09876543600922834)]\n"
     ]
    }
   ],
   "source": [
    "X_test = bench_spectro_tensors\n",
    "y_test = np_utils.to_categorical (np.array (src_bench_spectro_data['target']), 24)\n",
    "\n",
    "model.load_weights (os.path.join ('model','model.w.best.h5'))\n",
    "\n",
    "score = model.evaluate (X_test, y_test, verbose=1)\n",
    "print ([(model.metrics_names[i], score[i]) for i in range (len (model.metrics_names))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cm</th>\n",
       "      <th>C</th>\n",
       "      <th>C#m</th>\n",
       "      <th>C#</th>\n",
       "      <th>Dm</th>\n",
       "      <th>D</th>\n",
       "      <th>D#m</th>\n",
       "      <th>D#</th>\n",
       "      <th>Em</th>\n",
       "      <th>E</th>\n",
       "      <th>Fm</th>\n",
       "      <th>F</th>\n",
       "      <th>F#m</th>\n",
       "      <th>F#</th>\n",
       "      <th>Gm</th>\n",
       "      <th>G</th>\n",
       "      <th>G#m</th>\n",
       "      <th>G#</th>\n",
       "      <th>Am</th>\n",
       "      <th>A</th>\n",
       "      <th>A#m</th>\n",
       "      <th>A#</th>\n",
       "      <th>Bm</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cm</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Em</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Am</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A#m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cm  C  C#m  C#  Dm  D  D#m  D#  Em  E  Fm  F  F#m  F#  Gm  G  G#m  G#  \\\n",
       "Cm    2  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C     0  2    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C#m   0  0    2   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "C#    0  0    0   2   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "Dm    0  0    0   0   2  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "D     0  0    1   0   0  1    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "D#m   0  0    0   0   0  0    2   0   0  0   0  0    0   0   0  0    0   0   \n",
       "D#    0  0    0   0   0  0    0   1   0  0   0  0    0   0   0  0    0   0   \n",
       "Em    0  0    0   0   0  0    0   0   2  0   0  0    0   0   0  0    0   0   \n",
       "E     0  0    0   0   0  0    0   0   0  2   0  0    0   0   0  0    0   0   \n",
       "Fm    0  0    0   0   0  0    0   0   0  0   2  0    0   0   0  0    0   0   \n",
       "F     0  0    0   0   0  0    0   0   0  0   0  2    0   0   0  0    0   0   \n",
       "F#m   0  0    0   0   0  0    0   0   0  0   0  0    2   0   0  0    0   0   \n",
       "F#    0  0    0   0   0  0    0   0   0  0   0  0    0   2   0  0    0   0   \n",
       "Gm    0  0    0   0   0  0    0   0   0  0   0  0    0   0   1  0    0   0   \n",
       "G     0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  1    0   1   \n",
       "G#m   0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    2   0   \n",
       "G#    0  0    1   0   0  0    0   0   0  0   0  0    0   0   0  0    0   1   \n",
       "Am    0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "A     0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "A#m   0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "A#    0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  0    0   0   \n",
       "Bm    0  0    0   0   0  0    0   0   0  0   0  0    0   1   0  0    0   1   \n",
       "B     0  0    0   0   0  0    0   0   0  0   0  0    0   0   0  1    0   0   \n",
       "\n",
       "     Am  A  A#m  A#  Bm  B  \n",
       "Cm    0  0    0   0   0  0  \n",
       "C     0  0    0   0   0  0  \n",
       "C#m   0  0    0   0   0  0  \n",
       "C#    0  0    0   0   0  0  \n",
       "Dm    0  0    0   0   0  0  \n",
       "D     0  0    0   0   0  0  \n",
       "D#m   0  0    0   0   0  0  \n",
       "D#    0  1    0   0   0  0  \n",
       "Em    0  0    0   0   0  0  \n",
       "E     0  0    0   0   0  0  \n",
       "Fm    0  0    0   0   0  0  \n",
       "F     0  0    0   0   0  0  \n",
       "F#m   0  0    0   0   0  0  \n",
       "F#    0  0    0   0   0  0  \n",
       "Gm    0  0    0   0   0  1  \n",
       "G     0  0    0   0   0  0  \n",
       "G#m   0  0    0   0   0  0  \n",
       "G#    0  0    0   0   0  0  \n",
       "Am    1  1    0   0   0  0  \n",
       "A     0  2    0   0   0  0  \n",
       "A#m   0  0    2   0   0  0  \n",
       "A#    0  1    0   1   0  0  \n",
       "Bm    0  0    0   0   0  0  \n",
       "B     0  0    1   0   0  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [y_test[i].argmax () for i in range (len (y_test))]\n",
    "model_pred = model.predict (X_test)\n",
    "y_pred = [model_pred[i].argmax () for i in range (len (model_pred))]\n",
    "\n",
    "cm = confusion_matrix (y_true, y_pred)\n",
    "cm_pd = pd.DataFrame (data=cm, index=src_bench_spectro_data['DESCR'], columns=src_bench_spectro_data['DESCR'])\n",
    "\n",
    "pd.set_option ('display.max_columns', 24)\n",
    "display (cm_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scikitplot as skplt\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix (y_true, y_pred, figsize=(14,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> saving model architecture... done\n",
      ">>> saving model weights... done\n",
      ">>> saving model history... done\n"
     ]
    }
   ],
   "source": [
    "# serialization of model architecture\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "t = time.strftime (\"%Y%m%d-%H%M%S\", time.gmtime ())\n",
    "\n",
    "# save model architecture\n",
    "save_name = os.path.join ('model', t + '_model.arch' + '.yaml')\n",
    "print ('>>> saving model architecture...', end=' ', flush=True)\n",
    "yaml_string = model.to_yaml ()\n",
    "with open (save_name, 'w') as yaml_file:\n",
    "    yaml_file.write (yaml_string)\n",
    "print ('done')\n",
    "\n",
    "# save model weights\n",
    "save_name = os.path.join ('model', t + '_model.weights' + '.h5')\n",
    "print ('>>> saving model weights...', end=' ', flush=True)\n",
    "model.save_weights (save_name)\n",
    "print ('done')\n",
    "\n",
    "# save learning history\n",
    "save_name = os.path.join ('model', t + '_model.hist' + '.csv')\n",
    "print ('>>> saving model history...', end=' ', flush=True)\n",
    "hist_df = pd.DataFrame.from_dict (history.history)\n",
    "hist_df.to_csv (save_name)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> loading and compiling model... done\n",
      ">>> loading best weights into model... done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 32)      160       \n",
      "_________________________________________________________________\n",
      "maxp_1 (MaxPooling2D)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 80, 80, 64)        8256      \n",
      "_________________________________________________________________\n",
      "maxp_2 (MaxPooling2D)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 40, 128)       32896     \n",
      "_________________________________________________________________\n",
      "maxp_3 (MaxPooling2D)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "avg_flatten (GlobalAveragePo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 24)                3096      \n",
      "=================================================================\n",
      "Total params: 44,408\n",
      "Trainable params: 44,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model architecture\n",
    "import os\n",
    "from keras import models\n",
    "\n",
    "load_name = os.path.join ('model', 'model.arch_180902.yaml')\n",
    "print ('>>> loading and compiling model...', end=' ', flush=True)\n",
    "with open (load_name, 'r') as yaml_file:\n",
    "    yaml_string = yaml_file.read ()\n",
    "model = models.model_from_yaml (yaml_string)\n",
    "model.compile (optimizer=opt_sgd, loss=losses.mean_squared_error, metrics=[fbeta])\n",
    "print ('done')\n",
    "\n",
    "# load best weights\n",
    "print ('>>> loading best weights into model...', end=' ', flush=True)\n",
    "model.load_weights (os.path.join ('model','model.w.best_180902.h5'))\n",
    "print ('done')\n",
    "\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_spectro/5-1/TRCWNLD128F93011AD_0-oct.png\n",
      "y_true 5-1\n",
      "y_pred 5-1\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "test_file = src_spectro_data['filenames'][idx]\n",
    "\n",
    "test_spectro = path_to_tensor (test_file)\n",
    "test_pred = model.predict (test_spectro)\n",
    "\n",
    "print (test_file)\n",
    "print ('y_true', src_spectro_data['target_names'][src_spectro_data['target'][idx]])\n",
    "print ('y_pred', src_spectro_data['target_names'][test_pred.argmax ()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**drawbacks** (known, unresolvable issues)\n",
    "\n",
    "(WRONG) *music keys vs CNN key classes*\n",
    "\n",
    "See <a href='https://www.researchgate.net/publication/228963946_Audio_onset_detection_using_machine_learning_techniques_the_effect_and_applicability_of_key_and_tempo_information'>Chuan, Ching-Hua & Chew, Elaine. (2018). Audio onset detection using machine learning techniques: the effect and applicability of key and tempo information.</a>, p. 18\n",
    "\n",
    "The spectrograms show a pitch range given by the <a href='https://en.wikipedia.org/wiki/Scientific_pitch_notation#Table_of_note_frequencies'>Scientific Pitch Notation</a>. By that the range of notes goes from $C_{-1}$ = $0_{MIDI}$ up to $G_9$ = $127_{MIDI}$.\n",
    "\n",
    "Each note can be the tonic of a music key - for example the key 'C major' exists 11 times (ocatve -1 to 9). Thus the information of 128 keys is now squeezed into 24 key classes.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
